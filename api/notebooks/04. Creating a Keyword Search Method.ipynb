{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Keyword Search Method**\n",
    "I've gotten a prototype for \"neural\" search working pretty well, but I want to test out something a little more traditional: keyword search! Postgres apparently supports some full-text search capabilities, so I want to try them out within this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\api\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevb_b7z2dw1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "import time\n",
    "from pandas_gbq import read_gbq\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from IPython.display import Markdown, display\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.stats import kendalltau\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing modules custom-built for this project\n",
    "from utils.settings import (\n",
    "    GBQ_PROJECT_ID,\n",
    "    GBQ_DATASET_ID,\n",
    "    POSTGRES_USER,\n",
    "    POSTGRES_PASSWORD,\n",
    "    POSTGRES_HOST,\n",
    "    POSTGRES_PORT,\n",
    "    POSTGRES_DB,\n",
    "    LOG_TO_CONSOLE,\n",
    ")\n",
    "from utils.logging import get_logger\n",
    "from utils.postgres import query_postgres, upload_to_table\n",
    "from utils.postgres_queries import (\n",
    "    most_similar_embeddings_to_text_filtered,\n",
    "    create_filters,\n",
    "    get_most_similar_transcriptions_filtered,\n",
    "    fetch_text_for_segments\n",
    ")\n",
    "from utils.search import neural_search, keyword_search\n",
    "\n",
    "# Set up a logger for this notebook\n",
    "logger = get_logger(\"postgres_notebook\", log_to_console=LOG_TO_CONSOLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: we're going to set up the Postgres engine via SQLAlchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string to the database\n",
    "postgres_connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(postgres_connection_string)\n",
    "metadata = MetaData()\n",
    "session = sessionmaker(bind=engine)()\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensuring Full-Text Index Exits**\n",
    "One thing that I need to do: actually make sure that the `transcriptions` table contains a `tsvector` column, and that we have an index created for this column. This enables the full-text searching that I want to do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one row from the transcriptions table and convert it to a dictionary\n",
    "transcription_record_example = query_postgres(\"SELECT * FROM transcriptions LIMIT 1\", engine).iloc[0].to_dict()\n",
    "\n",
    "# Check to see if the ts_vec column is present in the transcriptions table\n",
    "tsvector_column_exists = \"ts_vec\" in transcription_record_example\n",
    "\n",
    "# Print the result\n",
    "print(f\"Does the ts_vec column exist in the transcriptions table? {tsvector_column_exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there isn't a `ts_vec` column within the `transcriptions` table, we'll create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're only going to run this notebook if the ts_vec column doesn't exist\n",
    "if not tsvector_column_exists:\n",
    "    # Define a query that'll add a tsvector column to the transcriptions table\n",
    "    add_tsvector_column_query = \"\"\"\n",
    "    ALTER TABLE transcriptions\n",
    "    ADD COLUMN ts_vec TSVECTOR\n",
    "    GENERATED ALWAYS AS (\n",
    "        to_tsvector('english', text) \n",
    "    ) STORED;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    query_postgres(add_tsvector_column_query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: we're going to check if there's an index on this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query to check if the index exists\n",
    "check_index_query = \"\"\"\n",
    "SELECT \n",
    "    EXISTS (\n",
    "        SELECT 1\n",
    "        FROM pg_class c\n",
    "        JOIN pg_namespace n ON n.oid = c.relnamespace\n",
    "        WHERE c.relname = 'transcriptions_text_idx'\n",
    "        AND n.nspname = 'public'\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the result\n",
    "index_exists = query_postgres(check_index_query, engine).iloc[0].exists\n",
    "\n",
    "# Print the result\n",
    "print(f\"Does the transcriptions_text_idx index exist? {index_exists}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the index doesn't exist, then we're going to add it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the index_exists variable is False, then we'll create the index\n",
    "if not index_exists:\n",
    "\n",
    "    # Set the maintenance_work_mem to 6GB\n",
    "    query_postgres(\"SET maintenance_work_mem = '6GB';\", engine)\n",
    "\n",
    "    # Create a GIN index on the ts_vec column\n",
    "    create_index_query = (\n",
    "        \"\"\"CREATE INDEX transcriptions_text_idx ON transcriptions USING GIN(ts_vec);\"\"\"\n",
    "    )\n",
    "\n",
    "    # Execute the query\n",
    "    query_postgres(create_index_query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing Full-Text Search**\n",
    "Now that I've got a `tsvector` column and a GIN index, I can test the full-text search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the function\n",
    "query = \"jim carrey\"\n",
    "release_date_filter = [datetime(2010, 1, 1), datetime(2023, 1, 1)]\n",
    "review_score_filter = [6, 9]\n",
    "video_type_filter = [\"album_review\"]\n",
    "n_results = 100\n",
    "n_most_similar_videos = 5\n",
    "n_top_segments_per_video = 3\n",
    "\n",
    "\n",
    "# Get the most similar transcriptions to the query\n",
    "most_similar_transcriptions_df = get_most_similar_transcriptions_filtered(\n",
    "    query=query,\n",
    "    engine=engine,\n",
    "    release_date_filter=release_date_filter,\n",
    "    review_score_filter=review_score_filter,\n",
    "    video_type_filter=video_type_filter,\n",
    "    n_results=n_results,\n",
    "    include_text=False\n",
    ")\n",
    "\n",
    "most_similar_transcriptions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got some of the most similar text results, I can aggregate some information about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the transcription stats\n",
    "aggregated_transcription_stats_df = most_similar_transcriptions_df.groupby(\"url\").agg(\n",
    "    median_rank=(\"rank\", \"median\"),\n",
    "    mean_rank=(\"rank\", \"mean\"),\n",
    "    n_results=(\"rank\", \"count\"),\n",
    ")\n",
    "\n",
    "# Determine the z-score of the count of results\n",
    "aggregated_transcription_stats_df[\"count_z_score\"] = (\n",
    "    aggregated_transcription_stats_df[\"n_results\"]\n",
    "    - aggregated_transcription_stats_df[\"n_results\"].mean()\n",
    ") / aggregated_transcription_stats_df[\"n_results\"].std()\n",
    "\n",
    "# Now, make a \"weighted median\", which is the median rank weighted by the z-score of the count of results\n",
    "aggregated_transcription_stats_df[\"weighted_median_rank\"] = (\n",
    "    aggregated_transcription_stats_df[\"median_rank\"]\n",
    "    * aggregated_transcription_stats_df[\"count_z_score\"]\n",
    ")\n",
    "\n",
    "# Sort the dataframe by the weighted median rank\n",
    "aggregated_transcription_stats_df = (\n",
    "    aggregated_transcription_stats_df.sort_values(\n",
    "        \"weighted_median_rank\", ascending=False\n",
    "    )\n",
    "    .head(n_most_similar_videos)\n",
    "    .reset_index()\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'm going to retrieve some video metadata and text. I'll start with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uplaod the dataframe to the database\n",
    "aggregated_transcription_stats_df.to_sql(\"most_similar_transcriptions_temp\", engine, if_exists=\"replace\")\n",
    "\n",
    "# Create a query to get the video metadata\n",
    "video_metadata_query = \"\"\"\n",
    "SELECT\n",
    "    video.*\n",
    "FROM\n",
    "    video_metadata video\n",
    "JOIN\n",
    "    most_similar_transcriptions_temp transcriptions\n",
    "ON\n",
    "    video.url = transcriptions.url\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "most_similar_videos_metadata_df = query_postgres(video_metadata_query, engine)\n",
    "\n",
    "# Re-join the aggreageted transcription stats with the video metadata\n",
    "aggregated_transcription_stats_with_metadata_df = aggregated_transcription_stats_df.merge(\n",
    "    most_similar_videos_metadata_df, on=\"url\"\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "aggregated_transcription_stats_with_metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_to_fetch_df = most_similar_transcriptions_df.merge(\n",
    "    aggregated_transcription_stats_with_metadata_df[[\"url\"]],\n",
    "    on=\"url\",\n",
    ")\n",
    "\n",
    "# Upload a temporary table to the database\n",
    "transcriptions_to_fetch_df.to_sql(\n",
    "    \"transcriptions_to_fetch_temp\", engine, if_exists=\"replace\"\n",
    ")\n",
    "\n",
    "# Now, we'll fetch the text for the segments\n",
    "fetch_text_for_segments_query = \"\"\"\n",
    "SELECT\n",
    "    transcriptions.text,\n",
    "    transcriptions.url,\n",
    "    transcriptions.segment_id AS id\n",
    "FROM\n",
    "    transcriptions_to_fetch_temp segments\n",
    "LEFT JOIN\n",
    "    transcriptions\n",
    "ON\n",
    "    transcriptions.url = segments.url\n",
    "    AND transcriptions.segment_id = segments.id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "transcriptions_with_text_df = query_postgres(fetch_text_for_segments_query, engine)\n",
    "\n",
    "# Aggregate this into the transcriptions_to_fetch_df dataframe\n",
    "transcriptions_to_fetch_df = transcriptions_to_fetch_df.merge(\n",
    "    transcriptions_with_text_df, on=[\"url\", \"id\"]\n",
    ")\n",
    "\n",
    "# Aggregate the text into a list\n",
    "transcriptions_to_fetch_df = (\n",
    "    transcriptions_to_fetch_df.sort_values(\"rank\", ascending=False)\n",
    "    .groupby(\"url\")\n",
    "    .agg(top_segment_chunks=(\"text\", lambda x: list(x)[:n_top_segments_per_video]))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add this to the aggregated_transcription_stats_with_metadata_df dataframe\n",
    "aggregated_transcription_stats_with_metadata_and_text_df = aggregated_transcription_stats_with_metadata_df.merge(\n",
    "    transcriptions_to_fetch_df, on=\"url\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can show off the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in aggregated_transcription_stats_with_metadata_and_text_df.head(3).iterrows():\n",
    "    display(Markdown(f\"**{row['title']}**\"))\n",
    "    display(Markdown(\"\\n\".join([f\"* {chunk}\" for chunk in row['top_segment_chunks']])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalized Version\n",
    "Now, I've gone ahead and turned this into a method of its own: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most_similar_transcription_segments_query is:\n",
      "\n",
      "\n",
      "    SELECT\n",
      "    video_metadata.title,\n",
      "    transcriptions.url,\n",
      "    \n",
      "    transcriptions.segment_id AS id,\n",
      "    transcriptions.segment_seek,\n",
      "    transcriptions.segment_start,\n",
      "    transcriptions.segment_end,\n",
      "    ts_rank_cd(ts_vec, to_tsquery('english', 'beautiful | piano | strings, | slipping | trees | dreams | beautiful <-> piano | piano <-> strings, | strings, <-> slipping | slipping <-> trees | trees <-> dreams | beautiful <-> piano <-> strings, | piano <-> strings, <-> slipping | strings, <-> slipping <-> trees | slipping <-> trees <-> dreams')) AS rank\n",
      "    FROM\n",
      "    transcriptions\n",
      "    LEFT JOIN\n",
      "    video_metadata\n",
      "    ON\n",
      "    video_metadata.url = transcriptions.url\n",
      "    WHERE\n",
      "    ts_vec @@ to_tsquery('english', 'beautiful | piano | strings, | slipping | trees | dreams | beautiful <-> piano | piano <-> strings, | strings, <-> slipping | slipping <-> trees | trees <-> dreams | beautiful <-> piano <-> strings, | piano <-> strings, <-> slipping | strings, <-> slipping <-> trees | slipping <-> trees <-> dreams')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Parameterize the function\n",
    "query = \"beautiful piano strings, slipping through the trees like dreams\"\n",
    "release_date_filter = [datetime(2010, 1, 1), datetime(2023, 1, 1)]\n",
    "review_score_filter = [6, 9]\n",
    "video_type_filter = [\"album_review\"]\n",
    "n_results = 100\n",
    "n_most_similar_videos = 5\n",
    "n_top_segments_per_video = 3\n",
    "\n",
    "release_date_filter = None\n",
    "review_score_filter = None\n",
    "video_type_filter = None\n",
    "\n",
    "# Run the query\n",
    "keyword_search_results_json = keyword_search(\n",
    "    query=query,\n",
    "    release_date_filter=release_date_filter,\n",
    "    review_score_filter=review_score_filter,\n",
    "    video_type_filter=video_type_filter,\n",
    "    n_results_to_consider=n_results,\n",
    "    n_most_similar_videos=n_most_similar_videos,\n",
    "    n_top_segments_per_video=n_top_segments_per_video,\n",
    ")\n",
    "\n",
    "# Convert to a dataframe\n",
    "keyword_search_results_df = pd.DataFrame(json.loads(keyword_search_results_json))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' This is a no frills, no bells and whistles just clean cut straight forward set of folkie, low key tunes with beautiful poetic songwriting killer singing the instrumentation again bear almost skeletal at points but still compelling still moving still emotionally potent this record is beautiful from corner to corner beautiful beautiful beautiful beautiful beautiful beautiful beautiful.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the results\n",
    "keyword_search_results_df.iloc[0].top_segment_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
