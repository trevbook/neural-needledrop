{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Hybrid Search Method**\n",
    "In this notebook, I'm going to try and combine the results of the neural search with the keyword search. This will result in a \"hybrid search\" algorithm, which combines the strengths of both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\api\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing modules custom-built for this project\n",
    "from utils.settings import (\n",
    "    LOG_TO_CONSOLE,\n",
    "    POSTGRES_DB, \n",
    "    POSTGRES_HOST, \n",
    "    POSTGRES_PASSWORD, \n",
    "    POSTGRES_PORT, \n",
    "    POSTGRES_USER\n",
    ")\n",
    "from utils.logging import get_logger\n",
    "from utils.search import neural_search, keyword_search, rerank_segment_chunks_for_urls\n",
    "import utils.postgres_queries as pg_queries\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up a logger for this notebook\n",
    "logger = get_logger(\"postgres_notebook\", log_to_console=LOG_TO_CONSOLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: we're going to set up the Postgres engine via SQLAlchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string to the database\n",
    "postgres_connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(postgres_connection_string)\n",
    "metadata = MetaData()\n",
    "session = sessionmaker(bind=engine)()\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Hybrid Search\n",
    "Below, I'm going to prototype the results of a hybrid search. I'll start by running the hybrid search itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the search\n",
    "query = \"jim carrey\"\n",
    "release_date_filter = None\n",
    "video_type_filter = [\"album_review\"]\n",
    "review_score_filter = [7, 10]\n",
    "max_video_per_search_method = 20\n",
    "max_results = 5\n",
    "keyword_weight = 1\n",
    "neural_weight = 0.6\n",
    "\n",
    "# Run the neural search and keyword search in parallel\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_neural = executor.submit(\n",
    "        neural_search,\n",
    "        query=query,\n",
    "        release_date_filter=release_date_filter,\n",
    "        video_type_filter=video_type_filter,\n",
    "        review_score_filter=review_score_filter,\n",
    "        n_videos_to_return=max_video_per_search_method,\n",
    "    )\n",
    "    future_keyword = executor.submit(\n",
    "        keyword_search,\n",
    "        query=query,\n",
    "        release_date_filter=release_date_filter,\n",
    "        video_type_filter=video_type_filter,\n",
    "        review_score_filter=review_score_filter,\n",
    "        n_most_similar_videos=max_video_per_search_method,\n",
    "    )\n",
    "\n",
    "# Get the results\n",
    "neural_results_json_str = future_neural.result()\n",
    "keyword_results_json_str = future_keyword.result()\n",
    "\n",
    "# Get DataFrame from the results\n",
    "neural_results_df = pd.DataFrame(json.loads(neural_results_json_str)).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "keyword_results_df = pd.DataFrame(json.loads(keyword_results_json_str)).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to create a DataFrame containing the different video metadata values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the metadata of the resulting videos' metadata\n",
    "all_resulting_videos_metadata_df = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            neural_results_df.drop(\n",
    "                columns=[\n",
    "                    \"top_segment_chunks\",\n",
    "                    \"n_segment_matches\",\n",
    "                    \"neural_search_score\",\n",
    "                    \"neural_search_score_z_score\",\n",
    "                ]\n",
    "            ).copy(),\n",
    "            keyword_results_df.drop(\n",
    "                columns=[\n",
    "                    \"top_segment_chunks\",\n",
    "                    \"n_segment_matches\",\n",
    "                    \"keyword_search_score\",\n",
    "                    \"keyword_search_score_z_score\",\n",
    "                ]\n",
    "            ).copy(),\n",
    "        ]\n",
    "    )\n",
    "    .drop_duplicates(subset=[\"url\"])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to do some \"reciprocal rank fusion\" - a pretty simple algorithm for merging the results of two rankings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>neural_score</th>\n",
       "      <th>neural_search_score_z_score</th>\n",
       "      <th>keyword_score</th>\n",
       "      <th>keyword_search_score_z_score</th>\n",
       "      <th>fused_score</th>\n",
       "      <th>avg_z_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.youtube.com/watch?v=LiQfSv1p_IA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com/watch?v=zfDFz8OmgzE</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>2.929927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>1.098723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.youtube.com/watch?v=nDZqxWjRLCU</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.062586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.398470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.youtube.com/watch?v=-deQEteXUK4</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.974586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.365470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.youtube.com/watch?v=74NdKIdbXJc</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.649602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.243601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url  neural_score  \\\n",
       "0  https://www.youtube.com/watch?v=LiQfSv1p_IA      0.000000   \n",
       "1  https://www.youtube.com/watch?v=zfDFz8OmgzE      0.983607   \n",
       "2  https://www.youtube.com/watch?v=nDZqxWjRLCU      0.967742   \n",
       "3  https://www.youtube.com/watch?v=-deQEteXUK4      0.952381   \n",
       "4  https://www.youtube.com/watch?v=74NdKIdbXJc      0.937500   \n",
       "\n",
       "   neural_search_score_z_score  keyword_score  keyword_search_score_z_score  \\\n",
       "0                     0.000000       0.983607                           2.0   \n",
       "1                     2.929927       0.000000                           0.0   \n",
       "2                     1.062586       0.000000                           0.0   \n",
       "3                     0.974586       0.000000                           0.0   \n",
       "4                     0.649602       0.000000                           0.0   \n",
       "\n",
       "   fused_score  avg_z_score  \n",
       "0     0.983607     1.250000  \n",
       "1     0.983607     1.098723  \n",
       "2     0.967742     0.398470  \n",
       "3     0.952381     0.365470  \n",
       "4     0.937500     0.243601  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some reciprocal rank fusion\n",
    "k = 60\n",
    "neural_results_df[\"neural_score\"] = k / (neural_results_df.index + 1 + k)\n",
    "keyword_results_df[\"keyword_score\"] = k / (keyword_results_df.index + 1 + k)\n",
    "fused_df = (\n",
    "    neural_results_df[[\"url\", \"neural_score\", \"neural_search_score_z_score\"]]\n",
    "    .copy()\n",
    "    .merge(\n",
    "        keyword_results_df[[\"url\", \"keyword_score\", \"keyword_search_score_z_score\"]],\n",
    "        on=\"url\",\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "fused_df[\"fused_score\"] = fused_df[\"neural_score\"] + fused_df[\"keyword_score\"]\n",
    "fused_df[\"avg_z_score\"] = (\n",
    "    fused_df[\"neural_search_score_z_score\"] * neural_weight\n",
    "    + fused_df[\"keyword_search_score_z_score\"] * keyword_weight\n",
    ") / (neural_weight + keyword_weight)\n",
    "fused_df = (\n",
    "    fused_df.sort_values(\n",
    "        [\n",
    "            \"fused_score\",\n",
    "            \"avg_z_score\",\n",
    "        ],\n",
    "        ascending=[False, False],\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .head(max_results)\n",
    ")\n",
    "\n",
    "fused_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got some top results, I'm going to do something a little more sophisticated: for each of the videos, I'm going to determine the best matches across *all* of the segment chunks for that video. Think of it as something of a \"segment re-ranking\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the reranking of the segment chunks\n",
    "reranked_segment_chunks_df = rerank_segment_chunks_for_urls(\n",
    "    query=query,\n",
    "    urls=fused_df[\"url\"].tolist(),\n",
    "    search_method=\"hybrid\",\n",
    "    n_top_segment_chunks=3\n",
    ")\n",
    "\n",
    "# Merge the reranked segment chunks with the fused_df\n",
    "fused_with_text_df = fused_df.merge(\n",
    "    reranked_segment_chunks_df,\n",
    "    on=\"url\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll merge this back with the `all_resulting_videos_metadata_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the metadata of the resulting videos' metadata\n",
    "result_df = fused_with_text_df.copy().merge(\n",
    "    all_resulting_videos_metadata_df, on=\"url\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalizing the Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevb_b7z2dw1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 neural results and 20 keyword results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\api\\utils\\search.py:497: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ]\n"
     ]
    }
   ],
   "source": [
    "from utils.search import hybrid_search\n",
    "\n",
    "hybrid_search_results_json_str = hybrid_search(\n",
    "    query=\"beautiful piano strings, slipping through the trees like dreams\",\n",
    "    release_date_filter=None,\n",
    "    video_type_filter=None,\n",
    "    review_score_filter=None,\n",
    "    max_video_per_search_method=20,\n",
    "    max_results=10,\n",
    "    keyword_weight=1,\n",
    "    neural_weight=0.85\n",
    ")\n",
    "\n",
    "hybrid_search_results_df = pd.DataFrame(json.loads(hybrid_search_results_json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>top_segment_chunks</th>\n",
       "      <th>neural_score</th>\n",
       "      <th>neural_search_score_z_score</th>\n",
       "      <th>keyword_score</th>\n",
       "      <th>keyword_search_score_z_score</th>\n",
       "      <th>fused_score</th>\n",
       "      <th>avg_z_score</th>\n",
       "      <th>cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TND Podcast #45 ft. FrankJavCee</td>\n",
       "      <td>[with little sketchings and you would tell you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>36.094834</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>19.510721</td>\n",
       "      <td>[0.3563498132, 0.3514914722, 0.3506569684]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TND Podcast #42 ft. Digibro</td>\n",
       "      <td>[but I love in the mountain in the cloud and, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>19.894517</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>10.753793</td>\n",
       "      <td>[0.312143445, 0.296794856, 0.2914485584]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TND Podcast #52 ft. Pyrocynical and NFKRZ</td>\n",
       "      <td>[It's got a chill wave vibe to it. You know, I...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>15.103193</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>8.163888</td>\n",
       "      <td>[0.3711246031, 0.3406806588, 0.2706609772]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TND Podcast #57: How To Get Into Death Grips f...</td>\n",
       "      <td>[I think that's what makes them so fresh and t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>11.888761</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>6.426357</td>\n",
       "      <td>[0.3680948458, 0.3656269094, 0.3617364764]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TND Podcast #55: clipping. Live Interview @ S....</td>\n",
       "      <td>[they can be and just kind of really kind of t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>11.834967</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>6.397279</td>\n",
       "      <td>[0.3549130049, 0.2874077942, 0.2858831499]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TND Podcast #17 ft. George Miller a.k.a. Filth...</td>\n",
       "      <td>[Yeah, absolutely. I discovered porn on new gr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11.555187</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6.246047</td>\n",
       "      <td>[0.2062668266, 0.1992480516, 0.1962147568]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TND Podcast #60: Worst Rappers in the Game 201...</td>\n",
       "      <td>[that's that's on his own music and I wasn't r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>11.160964</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>6.032954</td>\n",
       "      <td>[0.3201868129, 0.3192802883, 0.2916520238]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TND Podcast #58: How To Review An Album ft. Zo...</td>\n",
       "      <td>[to kind of stress to the listener kind of cat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>10.827391</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>5.852644</td>\n",
       "      <td>[0.2940289624, 0.2933736444, 0.2817878387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TND Podcast #54 ft. Adam of YourMovieSucks</td>\n",
       "      <td>[bit of Regina Spector in my album maybe a lit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>10.766742</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>5.819860</td>\n",
       "      <td>[0.3599791956, 0.3583581661, 0.3267615438]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TND Podcast #56: 10 Worst Rappers (Revisited) ...</td>\n",
       "      <td>[10 out of 10, bitch. Yeah. Maybe somebody can...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>8.734978</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4.721610</td>\n",
       "      <td>[0.3345420064, 0.2706295525, 0.2679424722]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                    TND Podcast #45 ft. FrankJavCee   \n",
       "1                        TND Podcast #42 ft. Digibro   \n",
       "2          TND Podcast #52 ft. Pyrocynical and NFKRZ   \n",
       "3  TND Podcast #57: How To Get Into Death Grips f...   \n",
       "4  TND Podcast #55: clipping. Live Interview @ S....   \n",
       "5  TND Podcast #17 ft. George Miller a.k.a. Filth...   \n",
       "6  TND Podcast #60: Worst Rappers in the Game 201...   \n",
       "7  TND Podcast #58: How To Review An Album ft. Zo...   \n",
       "8         TND Podcast #54 ft. Adam of YourMovieSucks   \n",
       "9  TND Podcast #56: 10 Worst Rappers (Revisited) ...   \n",
       "\n",
       "                                  top_segment_chunks  neural_score  \\\n",
       "0  [with little sketchings and you would tell you...           0.0   \n",
       "1  [but I love in the mountain in the cloud and, ...           0.0   \n",
       "2  [It's got a chill wave vibe to it. You know, I...           0.0   \n",
       "3  [I think that's what makes them so fresh and t...           0.0   \n",
       "4  [they can be and just kind of really kind of t...           0.0   \n",
       "5  [Yeah, absolutely. I discovered porn on new gr...           0.0   \n",
       "6  [that's that's on his own music and I wasn't r...           0.0   \n",
       "7  [to kind of stress to the listener kind of cat...           0.0   \n",
       "8  [bit of Regina Spector in my album maybe a lit...           0.0   \n",
       "9  [10 out of 10, bitch. Yeah. Maybe somebody can...           0.0   \n",
       "\n",
       "   neural_search_score_z_score  keyword_score  keyword_search_score_z_score  \\\n",
       "0                          0.0       0.983607                     36.094834   \n",
       "1                          0.0       0.967742                     19.894517   \n",
       "2                          0.0       0.952381                     15.103193   \n",
       "3                          0.0       0.937500                     11.888761   \n",
       "4                          0.0       0.923077                     11.834967   \n",
       "5                          0.0       0.909091                     11.555187   \n",
       "6                          0.0       0.895522                     11.160964   \n",
       "7                          0.0       0.882353                     10.827391   \n",
       "8                          0.0       0.869565                     10.766742   \n",
       "9                          0.0       0.857143                      8.734978   \n",
       "\n",
       "   fused_score  avg_z_score                                     cos_sim  \n",
       "0     0.983607    19.510721  [0.3563498132, 0.3514914722, 0.3506569684]  \n",
       "1     0.967742    10.753793    [0.312143445, 0.296794856, 0.2914485584]  \n",
       "2     0.952381     8.163888  [0.3711246031, 0.3406806588, 0.2706609772]  \n",
       "3     0.937500     6.426357  [0.3680948458, 0.3656269094, 0.3617364764]  \n",
       "4     0.923077     6.397279  [0.3549130049, 0.2874077942, 0.2858831499]  \n",
       "5     0.909091     6.246047  [0.2062668266, 0.1992480516, 0.1962147568]  \n",
       "6     0.895522     6.032954  [0.3201868129, 0.3192802883, 0.2916520238]  \n",
       "7     0.882353     5.852644  [0.2940289624, 0.2933736444, 0.2817878387]  \n",
       "8     0.869565     5.819860  [0.3599791956, 0.3583581661, 0.3267615438]  \n",
       "9     0.857143     4.721610  [0.3345420064, 0.2706295525, 0.2679424722]  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search_results_df[\n",
    "    [\n",
    "        \"title\",\n",
    "        \"top_segment_chunks\",\n",
    "        \"neural_score\",\n",
    "        \"neural_search_score_z_score\",\n",
    "        \"keyword_score\",\n",
    "        \"keyword_search_score_z_score\",\n",
    "        \"fused_score\",\n",
    "        \"avg_z_score\",\n",
    "        \"cos_sim\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick | brown | fox | quick <-> brown | brown <-> fox | quick <-> brown <-> fox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevb_b7z2dw1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Ensure you have the stopwords dataset downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def generate_tsquery(input_query, max_n=3):\n",
    "    # Tokenize the input string into words\n",
    "    words = input_query.split()\n",
    "\n",
    "    # Filter out stopwords\n",
    "    filtered_words = [\n",
    "        word for word in words if word.lower() not in stopwords.words(\"english\")\n",
    "    ]\n",
    "\n",
    "    # Generate n-grams for each n from 1 to max_n\n",
    "    phrases = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        for ngram in ngrams(filtered_words, n):\n",
    "            phrase = \" <-> \".join(ngram)  # Use <-> for phrase search in tsquery\n",
    "            phrases.append(phrase)\n",
    "\n",
    "    # Combine all phrases with the OR operator for tsquery\n",
    "    tsquery = \" | \".join(phrases)\n",
    "\n",
    "    return tsquery\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_query = \"the quick brown fox\"\n",
    "tsquery = generate_tsquery(input_query, 3)\n",
    "print(tsquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
