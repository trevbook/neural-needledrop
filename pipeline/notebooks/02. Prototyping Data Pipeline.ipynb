{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prototyping Data Pipeline**\n",
    "In this notebook, I'm going to be writing a bunch of functions that prototype a data pipeline for this app. Once I write the functions, I'll move them out of this notebook and into a utility file that the main pipeline script can also access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "from pytubefix import YouTube, Channel\n",
    "from google.cloud import bigquery\n",
    "import traceback\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas_gbq\n",
    "import datetime\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Importing custom utility functions\n",
    "import utils.gbq as gbq_utils\n",
    "import utils.youtube as youtube_utils\n",
    "import utils.gcs as gcs_utils\n",
    "\n",
    "# Indicate whether or not we want tqdm progress bars\n",
    "tqdm_enabled = True\n",
    "\n",
    "# Set some constants for the project\n",
    "GBQ_PROJECT_ID = \"neural-needledrop\"\n",
    "GBQ_DATASET_ID = \"backend_data\"\n",
    "\n",
    "# Set the pandas_gbq context to the project ID\n",
    "# pandas_gbq.context.project = GBQ_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking GBQ Table\n",
    "The **very** first thing I need to do: check the actual `video_metadata` GBQ table to determine the most recent video I've downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that'll grab the most recent video url\n",
    "most_recent_video_url_query = \"\"\"\n",
    "SELECT\n",
    "  metadata.url\n",
    "FROM\n",
    "  `neural-needledrop.backend_data.video_metadata` metadata\n",
    "ORDER BY\n",
    "  publish_date DESC, scrape_date DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Use pandas-gbq to run the query\n",
    "most_recent_video_url_df = pd.read_gbq(most_recent_video_url_query, project_id=GBQ_PROJECT_ID)\n",
    "\n",
    "# If the length of the dataframe is zero, then we need to set the url to None\n",
    "if len(most_recent_video_url_df) == 0:\n",
    "    most_recent_video_url = None\n",
    "\n",
    "# Otherwise, we can just grab the url from the dataframe\n",
    "else:\n",
    "    most_recent_video_url = most_recent_video_url_df.iloc[0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying New Videos\n",
    "The first portion of the pipeline: determining if there are any videos to work with in the first place! \n",
    "\n",
    "I'll start by parameterizing the method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the identification method\n",
    "video_limit = 250 # If video_limit is None, then we're going to download information for all of the videos\n",
    "\n",
    "# Define the channel of interest\n",
    "channel_url = \"https://www.youtube.com/c/theneedledrop\"\n",
    "\n",
    "most_recent_video_url = None\n",
    "\n",
    "# Indicate the step size for parsing the videos\n",
    "video_parse_step_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got the method scoped out, I'm going to write it. I'll identify the first couple of videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_urls_from_channel(channel, most_recent_video_url=None, video_limit=None, video_parse_step_size=10):\n",
    "    \"\"\"\n",
    "    Helper method to identify all of the video URLs from a channel.\n",
    "    If `most_recent_video_url` is None, then we're going to download information for all of the videos we can, \n",
    "    all the way up to the `video_limit`. If *that* is None, then we're going to download information for all of the videos.\n",
    "    The `video_parse_step_size` indicates how many videos we're going to parse at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the video URLs\n",
    "    video_urls = []\n",
    "    \n",
    "    # Initialize the video count\n",
    "    video_count = 0\n",
    "    \n",
    "    # Iterate through the channel's videos until we find the `most_recent_video_url`\n",
    "    while most_recent_video_url not in video_urls:\n",
    "        \n",
    "        # Fetch the video URLs\n",
    "        new_video_urls = channel.video_urls[video_count:video_count+video_parse_step_size]\n",
    "        \n",
    "        # Break out if no new video URLs were found\n",
    "        if len(new_video_urls) == 0:\n",
    "            break\n",
    "        \n",
    "        video_urls.extend(new_video_urls)\n",
    "        \n",
    "        # Update the video count\n",
    "        video_count += video_parse_step_size\n",
    "        \n",
    "        # If we've reached the video limit, then break\n",
    "        if (video_limit is not None and video_count >= video_limit):\n",
    "            break\n",
    "    \n",
    "    # Return the video URLs\n",
    "    return video_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method should function to do what I want. Let's test it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of video URLs we're going to parse\n",
    "video_urls_to_parse = get_video_urls_from_channel(\n",
    "    channel=Channel(channel_url),\n",
    "    most_recent_video_url=None,\n",
    "    video_limit=video_limit,\n",
    "    video_parse_step_size=video_parse_step_size,\n",
    ")\n",
    "\n",
    "# If the most_recent_video_url is not None, then we're going to remove all of the videos that come after it\n",
    "try:\n",
    "    if most_recent_video_url is not None:\n",
    "        video_urls_to_parse = video_urls_to_parse[\n",
    "            : video_urls_to_parse.index(most_recent_video_url)\n",
    "        ]\n",
    "    else:\n",
    "        pass\n",
    "# If we run into an error, then we're going to print out the traceback\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Print some information about the video URLs we're going to parse\n",
    "print(f\"Identified {len(video_urls_to_parse)} videos to parse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Already Parsed Videos\n",
    "We're going to upload a temporary table with all of the video URLs to GBQ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the video URLs\n",
    "video_urls_to_parse_df = pd.DataFrame(video_urls_to_parse, columns=[\"url\"])\n",
    "\n",
    "# Create a temporary table in GBQ\n",
    "temporary_table_name = gbq_utils.create_temporary_table_in_gbq(\n",
    "    dataframe=video_urls_to_parse_df,\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_name=GBQ_DATASET_ID,\n",
    "    table_name=\"temporary_video_urls_to_parse\",\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary table in hand, we'll query GBQ to figure out the *actual* videos to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query to identify the videos that we need to parse\n",
    "actual_videos_to_parse_query = f\"\"\"\n",
    "SELECT\n",
    "  temp_urls.url\n",
    "FROM\n",
    "  `{temporary_table_name}` temp_urls\n",
    "LEFT JOIN\n",
    "  `backend_data.video_metadata` metadata\n",
    "ON\n",
    "  metadata.url = temp_urls.url\n",
    "WHERE\n",
    "  metadata.id IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "actual_videos_to_parse_df = pd.read_gbq(\n",
    "    actual_videos_to_parse_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some cleanup: setting the `actual_videos_to_parse_df` contents to the video_urls_to_parse, and deleting the temporary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the videos we're going to parse\n",
    "print(f\"After filtering out videos that have already been parsed, we have {len(actual_videos_to_parse_df)} videos to parse.\")\n",
    "\n",
    "# Overriding the video_urls_to_parse with the contents of the actual_videos_to_parse_df\n",
    "video_urls_to_parse = list(actual_videos_to_parse_df[\"url\"])\n",
    "\n",
    "# Use the gbq_utils to delete the temporary table\n",
    "temp_table_project_id, temp_table_dataset_id, temp_table_name = temporary_table_name.split(\".\")\n",
    "gbq_utils.delete_table(\n",
    "    project_id=temp_table_project_id,\n",
    "    dataset_id=temp_table_dataset_id,\n",
    "    table_id=temp_table_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Video Metadata\n",
    "Below, I'm going to define a method that'll download a video's metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata_from_video(video_url):\n",
    "    \"\"\"\n",
    "    This method will parse a dictionary containing metadata from a video, given its URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a video object\n",
    "    video = YouTube(video_url)\n",
    "\n",
    "    # Keep a dictionary to keep track of the metadata we're interested in\n",
    "    video_metadata_dict = {}\n",
    "\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Parse the `videoDetails` from the video; this contains a lot of the metadata we're interested in\n",
    "        vid_info_dict = video.vid_info\n",
    "        video_info_dict = vid_info_dict.get(\"videoDetails\")\n",
    "\n",
    "    # If we run into an Exception this early on, we'll raise an Exception\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Error parsing video metadata for video {video_url}: '{e}'\\nTraceback is as follows:\\n{traceback.format_exc()}\"\n",
    "        )\n",
    "\n",
    "    # Extract different pieces of the video metadata\n",
    "    video_metadata_dict[\"id\"] = video_info_dict.get(\"videoId\")\n",
    "    video_metadata_dict[\"title\"] = video_info_dict.get(\"title\")\n",
    "    video_metadata_dict[\"length\"] = video_info_dict.get(\"lengthSeconds\")\n",
    "    video_metadata_dict[\"channel_id\"] = video_info_dict.get(\"channelId\")\n",
    "    video_metadata_dict[\"channel_name\"] = video_info_dict.get(\"author\")\n",
    "    video_metadata_dict[\"short_description\"] = video_info_dict.get(\"shortDescription\")\n",
    "    video_metadata_dict[\"view_ct\"] = video_info_dict.get(\"viewCount\")\n",
    "    video_metadata_dict[\"url\"] = video_info_dict.get(\"video_url\")\n",
    "    video_metadata_dict[\"small_thumbnail_url\"] = (\n",
    "        video_info_dict.get(\"thumbnail\").get(\"thumbnails\")[0].get(\"url\")\n",
    "    )\n",
    "    video_metadata_dict[\"large_thumbnail_url\"] = (\n",
    "        video_info_dict.get(\"thumbnail\").get(\"thumbnails\")[-1].get(\"url\")\n",
    "    )\n",
    "\n",
    "    # Try and extract the the publish_date\n",
    "    try:\n",
    "        publish_date = video.publish_date\n",
    "        video_metadata_dict[\"publish_date\"] = publish_date\n",
    "    except:\n",
    "        video_metadata_dict[\"publish_date\"] = None\n",
    "\n",
    "    # Try and extract the full description\n",
    "    try:\n",
    "        full_description = video.description\n",
    "        video_metadata_dict[\"description\"] = full_description\n",
    "    except:\n",
    "        video_metadata_dict[\"description\"] = None\n",
    "    \n",
    "    # Use datetime to get the scrape_date (the current datetime)\n",
    "    video_metadata_dict[\"scrape_date\"] = datetime.datetime.now()\n",
    "    \n",
    "    # Add the url to the video_metadata_dict\n",
    "    video_metadata_dict[\"url\"] = video_url\n",
    "\n",
    "    # Finally, return the video metadata dictionary\n",
    "    return video_metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: we'll need to iterate through each of the videos and download their metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the video metadata parsing\n",
    "time_to_sleep_between_parsing = 2\n",
    "sleep_randomization_factor = 2.5\n",
    "\n",
    "# We'll iterate through each of the videos in the list and parse their metadata\n",
    "video_metadata_dicts_by_video_url = {}\n",
    "for video_url in tqdm(video_urls_to_parse, disable=not tqdm_enabled):\n",
    "    \n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Parse the metadata from the video\n",
    "        video_metadata_dict = parse_metadata_from_video(video_url)\n",
    "        \n",
    "        # Add the video metadata dictionary to the dictionary of video metadata dictionaries\n",
    "        video_metadata_dicts_by_video_url[video_url] = video_metadata_dict\n",
    "        \n",
    "        # Sleep for a random amount of time\n",
    "        time_to_sleep = random.uniform(time_to_sleep_between_parsing, time_to_sleep_between_parsing + (sleep_randomization_factor * time_to_sleep_between_parsing))\n",
    "        time.sleep(time_to_sleep)\n",
    "    \n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the Metadata\n",
    "Now that I've downloaded some metadata about different videos, I need to store it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the rows to add to the table\n",
    "rows_to_add = [val for val in video_metadata_dicts_by_video_url.values()]\n",
    "\n",
    "# Add the rows to the table\n",
    "gbq_utils.add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"video_metadata\",\n",
    "    rows=rows_to_add   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Video Audio\n",
    "Next up, I need to download some video audio. This one probably needs to go a lot slower than the metadata fetching 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Audio to Download\n",
    "I need to check with GBQ to see if there are any videos that I need to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the query\n",
    "n_max_video_urls = 2\n",
    "\n",
    "# The query below will determine which videos we need to download audio for\n",
    "videos_for_audio_parsing_query = f\"\"\"\n",
    "SELECT\n",
    "  video.url\n",
    "FROM\n",
    "  `backend_data.video_metadata` video\n",
    "LEFT JOIN\n",
    "  `backend_data.audio` audio\n",
    "ON\n",
    "  audio.video_url = video.url\n",
    "WHERE\n",
    "  audio.audio_gcs_uri IS NULL\n",
    "LIMIT {n_max_video_urls}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "videos_for_audio_parsing_df = pd.read_gbq(\n",
    "    videos_for_audio_parsing_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Audio\n",
    "Next, I'm going to use `pytube` to download the audio of these videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the download\n",
    "time_to_sleep_between_downloads = 10\n",
    "sleep_randomization_factor = 2.5\n",
    "download_directory = Path(\"temp_data/\")\n",
    "\n",
    "# Iterate through the videos and download their audio\n",
    "for video_url in tqdm(videos_for_audio_parsing_df[\"url\"], disable=not tqdm_enabled):\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Download the audio from the video\n",
    "        youtube_utils.download_audio_from_video(\n",
    "            video_url=video_url, data_folder_path=download_directory\n",
    "        )\n",
    "\n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Audio to GCS\n",
    "Now that I've downloaded the audio, I need to upload it to GCS. \n",
    "\n",
    "I'll start by creating the bucket if it doesn't exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the neural-needledrop-audio bucket exists\n",
    "gcs_utils.create_bucket(\n",
    "    \"neural-needledrop-audio\", project_id=GBQ_PROJECT_ID, delete_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: upload all of the audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the audio upload process\n",
    "delete_files_after_upload = False\n",
    "\n",
    "# Iterate through all of the video urls in the videos_for_audio_parsing_df\n",
    "for row in tqdm(\n",
    "    list(videos_for_audio_parsing_df.itertuples()), disable=not tqdm_enabled\n",
    "):\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Get the video url\n",
    "        video_url = row.url\n",
    "\n",
    "        # Get the video id\n",
    "        video_id = video_url.split(\"watch?v=\")[-1]\n",
    "\n",
    "        # Get the path to the audio file\n",
    "        audio_file_path = download_directory / f\"{video_id}.m4a\"\n",
    "\n",
    "        # Check to see if this file exists\n",
    "        if not Path(audio_file_path).exists():\n",
    "            # If it doesn't exist, then we'll continue. Print out a warning\n",
    "            print(f\"Warning: {audio_file_path} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get the GCS URI\n",
    "        gcs_uri = f\"neural-needledrop-audio\"\n",
    "\n",
    "        # Upload the audio file to GCS\n",
    "        audio_file_path_str = str(audio_file_path)\n",
    "        gcs_utils.upload_file_to_bucket(\n",
    "            file_path=audio_file_path_str,\n",
    "            bucket_name=gcs_uri,\n",
    "            project_id=GBQ_PROJECT_ID,\n",
    "        )\n",
    "\n",
    "        # Create a dictionary to store the audio metadata\n",
    "        audio_metadata_dict = {\n",
    "            \"video_url\": video_url,\n",
    "            \"audio_gcs_uri\": f\"gs://{gcs_uri}/{video_id}.m4a\",\n",
    "            \"scrape_date\": datetime.datetime.now(),\n",
    "        }\n",
    "\n",
    "        # Add the audio metadata to the table\n",
    "        try:\n",
    "            gbq_utils.add_rows_to_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                table_id=\"audio\",\n",
    "                rows=[audio_metadata_dict],\n",
    "            )\n",
    "        except NotFound:\n",
    "            gbq_utils.generate_audio_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                delete_if_exists=False,\n",
    "            )\n",
    "            gbq_utils.add_rows_to_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                table_id=\"audio\",\n",
    "                rows=[audio_metadata_dict],\n",
    "            )\n",
    "\n",
    "        # Delete the audio file if delete_files_after_upload\n",
    "        if delete_files_after_upload:\n",
    "            audio_file_path.unlink()\n",
    "\n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "# If we're deleting the files after upload, then we'll delete the download_directory\n",
    "if delete_files_after_upload:\n",
    "    Path(download_directory).rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a super quick fix that I ought to handle: I'm going to deduplicate the `backend_data.audio` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will deduplicate the audio table\n",
    "deduplicate_audio_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.audio` AS (\n",
    "    SELECT\n",
    "        video_url,\n",
    "        audio_gcs_uri,\n",
    "        scrape_date\n",
    "    FROM (\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY video_url ORDER BY scrape_date DESC) AS row_num\n",
    "        FROM\n",
    "            `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.audio`\n",
    "    ) ordered_table\n",
    "    WHERE\n",
    "        ordered_table.row_num = 1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "pandas_gbq.read_gbq(deduplicate_audio_table_query, project_id=GBQ_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing Audio with Whisper\n",
    "Now that I've downloaded some audio, I need to figure out what needs to be transcribed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
