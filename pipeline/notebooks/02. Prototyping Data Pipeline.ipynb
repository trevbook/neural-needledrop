{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prototyping Data Pipeline**\n",
    "In this notebook, I'm going to be writing a bunch of functions that prototype a data pipeline for this app. Once I write the functions, I'll move them out of this notebook and into a utility file that the main pipeline script can also access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "from pytubefix import YouTube, Channel\n",
    "from google.cloud import bigquery, storage\n",
    "import traceback\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas_gbq\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from google.cloud.exceptions import NotFound\n",
    "import whisper\n",
    "import numpy as np\n",
    "\n",
    "# Importing custom utility functions\n",
    "import utils.gbq as gbq_utils\n",
    "import utils.youtube as youtube_utils\n",
    "import utils.gcs as gcs_utils\n",
    "import utils.logging as log_utils\n",
    "import utils.enriching_data as enrichment_utils\n",
    "import utils.openai as openai_utils\n",
    "\n",
    "# Indicate whether or not we want tqdm progress bars\n",
    "tqdm_enabled = True\n",
    "\n",
    "# Set some constants for the project\n",
    "GBQ_PROJECT_ID = \"neural-needledrop\"\n",
    "GBQ_DATASET_ID = \"backend_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to configure the logger for the pipeline below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger\n",
    "logger = log_utils.get_logger(name=\"pipeline\", log_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load in a whisper model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Whisper model of choice\n",
    "whisper_model_size = \"tiny\"\n",
    "whisper_model = whisper.load_model(whisper_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking GBQ Table\n",
    "The **very** first thing I need to do: check the actual `video_metadata` GBQ table to determine the most recent video I've downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that'll grab the most recent video url\n",
    "most_recent_video_url_query = \"\"\"\n",
    "SELECT\n",
    "  metadata.url\n",
    "FROM\n",
    "  `neural-needledrop.backend_data.video_metadata` metadata\n",
    "ORDER BY\n",
    "  publish_date DESC, scrape_date DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Use pandas-gbq to run the query\n",
    "most_recent_video_url_df = pd.read_gbq(most_recent_video_url_query, project_id=GBQ_PROJECT_ID)\n",
    "\n",
    "# If the length of the dataframe is zero, then we need to set the url to None\n",
    "if len(most_recent_video_url_df) == 0:\n",
    "    most_recent_video_url = None\n",
    "\n",
    "# Otherwise, we can just grab the url from the dataframe\n",
    "else:\n",
    "    most_recent_video_url = most_recent_video_url_df.iloc[0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying New Videos\n",
    "The first portion of the pipeline: determining if there are any videos to work with in the first place! \n",
    "\n",
    "I'll start by parameterizing the method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the identification method\n",
    "video_limit = 1000 # If video_limit is None, then we're going to download information for all of the videos\n",
    "\n",
    "# Define the channel of interest\n",
    "channel_url = \"https://www.youtube.com/c/theneedledrop\"\n",
    "\n",
    "most_recent_video_url = None\n",
    "\n",
    "# Indicate the step size for parsing the videos\n",
    "video_parse_step_size = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got the method scoped out, I'm going to write it. I'll identify the first couple of videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_urls_from_channel(channel, most_recent_video_url=None, video_limit=None, video_parse_step_size=10):\n",
    "    \"\"\"\n",
    "    Helper method to identify all of the video URLs from a channel.\n",
    "    If `most_recent_video_url` is None, then we're going to download information for all of the videos we can, \n",
    "    all the way up to the `video_limit`. If *that* is None, then we're going to download information for all of the videos.\n",
    "    The `video_parse_step_size` indicates how many videos we're going to parse at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the video URLs\n",
    "    video_urls = []\n",
    "    \n",
    "    # Initialize the video count\n",
    "    video_count = 0\n",
    "    \n",
    "    # Iterate through the channel's videos until we find the `most_recent_video_url`\n",
    "    while most_recent_video_url not in video_urls:\n",
    "        \n",
    "        # Fetch the video URLs\n",
    "        new_video_urls = channel.video_urls[video_count:video_count+video_parse_step_size]\n",
    "        \n",
    "        # Break out if no new video URLs were found\n",
    "        if len(new_video_urls) == 0:\n",
    "            break\n",
    "        \n",
    "        video_urls.extend(new_video_urls)\n",
    "        \n",
    "        # Update the video count\n",
    "        video_count += video_parse_step_size\n",
    "        \n",
    "        # If we've reached the video limit, then break\n",
    "        if (video_limit is not None and video_count >= video_limit):\n",
    "            break\n",
    "    \n",
    "    # Return the video URLs\n",
    "    return video_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method should function to do what I want. Let's test it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of video URLs we're going to parse\n",
    "video_urls_to_parse = get_video_urls_from_channel(\n",
    "    channel=Channel(channel_url),\n",
    "    most_recent_video_url=None,\n",
    "    video_limit=video_limit,\n",
    "    video_parse_step_size=video_parse_step_size,\n",
    ")\n",
    "\n",
    "# If the most_recent_video_url is not None, then we're going to remove all of the videos that come after it\n",
    "try:\n",
    "    if most_recent_video_url is not None:\n",
    "        video_urls_to_parse = video_urls_to_parse[\n",
    "            : video_urls_to_parse.index(most_recent_video_url)\n",
    "        ]\n",
    "    else:\n",
    "        pass\n",
    "# If we run into an error, then we're going to print out the traceback\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Print some information about the video URLs we're going to parse\n",
    "print(f\"Identified {len(video_urls_to_parse)} videos to parse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Already Parsed Videos\n",
    "We're going to upload a temporary table with all of the video URLs to GBQ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the video URLs\n",
    "video_urls_to_parse_df = pd.DataFrame(video_urls_to_parse, columns=[\"url\"])\n",
    "\n",
    "# Create a temporary table in GBQ\n",
    "temporary_table_name = gbq_utils.create_temporary_table_in_gbq(\n",
    "    dataframe=video_urls_to_parse_df,\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_name=GBQ_DATASET_ID,\n",
    "    table_name=\"temporary_video_urls_to_parse\",\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this temporary table in hand, we'll query GBQ to figure out the *actual* videos to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query to identify the videos that we need to parse\n",
    "actual_videos_to_parse_query = f\"\"\"\n",
    "SELECT\n",
    "  temp_urls.url\n",
    "FROM\n",
    "  `{temporary_table_name}` temp_urls\n",
    "LEFT JOIN\n",
    "  `backend_data.video_metadata` metadata\n",
    "ON\n",
    "  metadata.url = temp_urls.url\n",
    "WHERE\n",
    "  metadata.id IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "actual_videos_to_parse_df = pd.read_gbq(\n",
    "    actual_videos_to_parse_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some cleanup: setting the `actual_videos_to_parse_df` contents to the video_urls_to_parse, and deleting the temporary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the videos we're going to parse\n",
    "print(f\"After filtering out videos that have already been parsed, we have {len(actual_videos_to_parse_df)} videos to parse.\")\n",
    "\n",
    "# Overriding the video_urls_to_parse with the contents of the actual_videos_to_parse_df\n",
    "video_urls_to_parse = list(actual_videos_to_parse_df[\"url\"])\n",
    "\n",
    "# Use the gbq_utils to delete the temporary table\n",
    "temp_table_project_id, temp_table_dataset_id, temp_table_name = temporary_table_name.split(\".\")\n",
    "gbq_utils.delete_table(\n",
    "    project_id=temp_table_project_id,\n",
    "    dataset_id=temp_table_dataset_id,\n",
    "    table_id=temp_table_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Video Metadata\n",
    "Below, I'm going to define a method that'll download a video's metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata_from_video(video_url):\n",
    "    \"\"\"\n",
    "    This method will parse a dictionary containing metadata from a video, given its URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a video object\n",
    "    video = YouTube(video_url)\n",
    "\n",
    "    # Keep a dictionary to keep track of the metadata we're interested in\n",
    "    video_metadata_dict = {}\n",
    "\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Parse the `videoDetails` from the video; this contains a lot of the metadata we're interested in\n",
    "        vid_info_dict = video.vid_info\n",
    "        video_info_dict = vid_info_dict.get(\"videoDetails\")\n",
    "\n",
    "    # If we run into an Exception this early on, we'll raise an Exception\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Error parsing video metadata for video {video_url}: '{e}'\\nTraceback is as follows:\\n{traceback.format_exc()}\"\n",
    "        )\n",
    "\n",
    "    # Extract different pieces of the video metadata\n",
    "    video_metadata_dict[\"id\"] = video_info_dict.get(\"videoId\")\n",
    "    video_metadata_dict[\"title\"] = video_info_dict.get(\"title\")\n",
    "    video_metadata_dict[\"length\"] = video_info_dict.get(\"lengthSeconds\")\n",
    "    video_metadata_dict[\"channel_id\"] = video_info_dict.get(\"channelId\")\n",
    "    video_metadata_dict[\"channel_name\"] = video_info_dict.get(\"author\")\n",
    "    video_metadata_dict[\"short_description\"] = video_info_dict.get(\"shortDescription\")\n",
    "    video_metadata_dict[\"view_ct\"] = video_info_dict.get(\"viewCount\")\n",
    "    video_metadata_dict[\"url\"] = video_info_dict.get(\"video_url\")\n",
    "    video_metadata_dict[\"small_thumbnail_url\"] = (\n",
    "        video_info_dict.get(\"thumbnail\").get(\"thumbnails\")[0].get(\"url\")\n",
    "    )\n",
    "    video_metadata_dict[\"large_thumbnail_url\"] = (\n",
    "        video_info_dict.get(\"thumbnail\").get(\"thumbnails\")[-1].get(\"url\")\n",
    "    )\n",
    "\n",
    "    # Try and extract the the publish_date\n",
    "    try:\n",
    "        publish_date = video.publish_date\n",
    "        video_metadata_dict[\"publish_date\"] = publish_date\n",
    "    except:\n",
    "        video_metadata_dict[\"publish_date\"] = None\n",
    "\n",
    "    # Try and extract the full description\n",
    "    try:\n",
    "        full_description = video.description\n",
    "        video_metadata_dict[\"description\"] = full_description\n",
    "    except:\n",
    "        video_metadata_dict[\"description\"] = None\n",
    "    \n",
    "    # Use datetime to get the scrape_date (the current datetime)\n",
    "    video_metadata_dict[\"scrape_date\"] = datetime.datetime.now()\n",
    "    \n",
    "    # Add the url to the video_metadata_dict\n",
    "    video_metadata_dict[\"url\"] = video_url\n",
    "\n",
    "    # Finally, return the video metadata dictionary\n",
    "    return video_metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: we'll need to iterate through each of the videos and download their metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the video metadata parsing\n",
    "time_to_sleep_between_parsing = 5\n",
    "sleep_randomization_factor = 3.5\n",
    "\n",
    "# We'll iterate through each of the videos in the list and parse their metadata\n",
    "video_metadata_dicts_by_video_url = {}\n",
    "for video_url in tqdm(video_urls_to_parse, disable=not tqdm_enabled):\n",
    "    \n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Parse the metadata from the video\n",
    "        video_metadata_dict = parse_metadata_from_video(video_url)\n",
    "        \n",
    "        # Add the video metadata dictionary to the dictionary of video metadata dictionaries\n",
    "        video_metadata_dicts_by_video_url[video_url] = video_metadata_dict\n",
    "        \n",
    "        # Sleep for a random amount of time\n",
    "        time_to_sleep = random.uniform(time_to_sleep_between_parsing, time_to_sleep_between_parsing + (sleep_randomization_factor * time_to_sleep_between_parsing))\n",
    "        time.sleep(time_to_sleep)\n",
    "    \n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the Metadata\n",
    "Now that I've downloaded some metadata about different videos, I need to store it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the rows to add to the table\n",
    "rows_to_add = [val for val in video_metadata_dicts_by_video_url.values()]\n",
    "\n",
    "# Add the rows to the table\n",
    "gbq_utils.add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"video_metadata\",\n",
    "    rows=rows_to_add   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Video Audio\n",
    "Next up, I need to download some video audio. This one probably needs to go a lot slower than the metadata fetching ðŸ˜…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Audio to Download\n",
    "I need to check with GBQ to see if there are any videos that I need to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the query\n",
    "n_max_video_urls = 50\n",
    "\n",
    "# The query below will determine which videos we need to download audio for\n",
    "videos_for_audio_parsing_query = f\"\"\"\n",
    "SELECT\n",
    "  video.url\n",
    "FROM\n",
    "  `backend_data.video_metadata` video\n",
    "LEFT JOIN\n",
    "  `backend_data.audio` audio\n",
    "ON\n",
    "  audio.video_url = video.url\n",
    "WHERE\n",
    "  audio.audio_gcs_uri IS NULL\n",
    "LIMIT {n_max_video_urls}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "videos_for_audio_parsing_df = pd.read_gbq(\n",
    "    videos_for_audio_parsing_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Audio\n",
    "Next, I'm going to use `pytube` to download the audio of these videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the download\n",
    "time_to_sleep_between_downloads = 25\n",
    "sleep_randomization_factor = 3.5\n",
    "download_directory = Path(\"temp_data/\")\n",
    "\n",
    "# Iterate through the videos and download their audio\n",
    "for video_url in tqdm(videos_for_audio_parsing_df[\"url\"], disable=not tqdm_enabled):\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Download the audio from the video\n",
    "        youtube_utils.download_audio_from_video(\n",
    "            video_url=video_url, data_folder_path=download_directory\n",
    "        )\n",
    "\n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Audio to GCS\n",
    "Now that I've downloaded the audio, I need to upload it to GCS. \n",
    "\n",
    "I'll start by creating the bucket if it doesn't exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the neural-needledrop-audio bucket exists\n",
    "gcs_utils.create_bucket(\n",
    "    \"neural-needledrop-audio\", project_id=GBQ_PROJECT_ID, delete_if_exists=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: upload all of the audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the audio upload process\n",
    "delete_files_after_upload = True\n",
    "\n",
    "# Iterate through all of the video urls in the videos_for_audio_parsing_df\n",
    "for row in tqdm(\n",
    "    list(videos_for_audio_parsing_df.itertuples()), disable=not tqdm_enabled\n",
    "):\n",
    "    # We'll wrap this in a try/except block so that we can catch any errors that occur\n",
    "    try:\n",
    "        # Get the video url\n",
    "        video_url = row.url\n",
    "\n",
    "        # Get the video id\n",
    "        video_id = video_url.split(\"watch?v=\")[-1]\n",
    "\n",
    "        # Get the path to the audio file\n",
    "        audio_file_path = download_directory / f\"{video_id}.m4a\"\n",
    "\n",
    "        # Check to see if this file exists\n",
    "        if not Path(audio_file_path).exists():\n",
    "            # If it doesn't exist, then we'll continue. Print out a warning\n",
    "            print(f\"Warning: {audio_file_path} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get the GCS URI\n",
    "        gcs_uri = f\"neural-needledrop-audio\"\n",
    "\n",
    "        # Upload the audio file to GCS\n",
    "        audio_file_path_str = str(audio_file_path)\n",
    "\n",
    "        # Convert the audio file to .mp3 using youtube_utils\n",
    "        youtube_utils.convert_m4a_to_mp3(\n",
    "            input_file_path=audio_file_path_str,\n",
    "            output_file_path=audio_file_path_str.replace(\".m4a\", \".mp3\"),\n",
    "        )\n",
    "\n",
    "        # Remove the .m4a file\n",
    "        audio_file_path.unlink()\n",
    "\n",
    "        # Update the audio_file_path_str\n",
    "        audio_file_path = Path(audio_file_path_str.replace(\".m4a\", \".mp3\"))\n",
    "        audio_file_path_str = str(audio_file_path)\n",
    "\n",
    "        gcs_utils.upload_file_to_bucket(\n",
    "            file_path=audio_file_path_str,\n",
    "            bucket_name=gcs_uri,\n",
    "            project_id=GBQ_PROJECT_ID,\n",
    "        )\n",
    "\n",
    "        # Create a dictionary to store the audio metadata\n",
    "        audio_metadata_dict = {\n",
    "            \"video_url\": video_url,\n",
    "            \"audio_gcs_uri\": f\"gs://{gcs_uri}/{audio_file_path.name}\",\n",
    "            \"scrape_date\": datetime.datetime.now(),\n",
    "        }\n",
    "\n",
    "        # Add the audio metadata to the table\n",
    "        try:\n",
    "            gbq_utils.add_rows_to_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                table_id=\"audio\",\n",
    "                rows=[audio_metadata_dict],\n",
    "            )\n",
    "        except NotFound:\n",
    "            gbq_utils.generate_audio_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                delete_if_exists=False,\n",
    "            )\n",
    "            gbq_utils.add_rows_to_table(\n",
    "                project_id=GBQ_PROJECT_ID,\n",
    "                dataset_id=GBQ_DATASET_ID,\n",
    "                table_id=\"audio\",\n",
    "                rows=[audio_metadata_dict],\n",
    "            )\n",
    "\n",
    "        # Delete the audio file if delete_files_after_upload\n",
    "        if delete_files_after_upload:\n",
    "            audio_file_path.unlink()\n",
    "\n",
    "    # If we run into an Exception, then we'll print out the traceback\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "# If we're deleting the files after upload, then we'll delete the download_directory\n",
    "if delete_files_after_upload:\n",
    "    Path(download_directory).rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a super quick fix that I ought to handle: I'm going to deduplicate the `backend_data.audio` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will deduplicate the audio table\n",
    "deduplicate_audio_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.audio` AS (\n",
    "    SELECT\n",
    "        video_url,\n",
    "        audio_gcs_uri,\n",
    "        scrape_date\n",
    "    FROM (\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY video_url ORDER BY scrape_date DESC) AS row_num\n",
    "        FROM\n",
    "            `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.audio`\n",
    "    ) ordered_table\n",
    "    WHERE\n",
    "        ordered_table.row_num = 1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "pandas_gbq.read_gbq(deduplicate_audio_table_query, project_id=GBQ_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing Audio with Whisper\n",
    "Now that I've downloaded some audio, I need to figure out what needs to be transcribed. I can do that by checking the `audio` and `transcriptions` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the transcription\n",
    "n_max_to_transcribe = 3\n",
    "\n",
    "# This query will determine all of the videos we need to transcribe\n",
    "videos_for_transcription_query = f\"\"\"\n",
    "SELECT\n",
    "  DISTINCT(audio.video_url) AS url,\n",
    "  audio.audio_gcr_uri\n",
    "FROM\n",
    "  `backend_data.audio` audio \n",
    "LEFT JOIN\n",
    "  `backend_data.transcriptions` transcript\n",
    "ON\n",
    "  audio.video_url = transcript.url\n",
    "WHERE\n",
    "  transcript.created_at IS NULL\n",
    "LIMIT {n_max_to_transcribe}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "videos_for_transcription_df = pd.read_gbq(\n",
    "    videos_for_transcription_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with all of the audio specified, we need to try and download it from `GCS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all of the video urls in the videos_for_transcription_df\n",
    "for row in tqdm(\n",
    "    list(videos_for_transcription_df.itertuples()), disable=not tqdm_enabled\n",
    "):\n",
    "    # Parse the GCS URI\n",
    "    split_gcs_uri = row.audio_gcr_uri.split(\"gs://\")[-1]\n",
    "    bucket_name, file_name = split_gcs_uri.split(\"/\")[0], \"/\".join(\n",
    "        split_gcs_uri.split(\"/\")[1:]\n",
    "    )\n",
    "\n",
    "    # Download the audio\n",
    "    gcs_utils.download_file_from_bucket(\n",
    "        bucket_name=bucket_name,\n",
    "        file_name=file_name,\n",
    "        destination_folder=\"temp_data/\",\n",
    "        project_id=GBQ_PROJECT_ID,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll store the audio metadata in a dictionary\n",
    "audio_metadata_dict_by_video_url = {}\n",
    "\n",
    "# Iterate through each of the files in the `temp_data` directory and transcribe them\n",
    "for child_file in tqdm(list(Path(\"temp_data/\").iterdir()), disable=not tqdm_enabled):\n",
    "    try:\n",
    "        if child_file.suffix != \".mp3\":\n",
    "            continue\n",
    "\n",
    "        # Extract some data about the file\n",
    "        video_url = f\"https://www.youtube.com/watch?v={child_file.stem}\"\n",
    "        video_id = child_file.stem\n",
    "\n",
    "        # Use whisper to transcribe the audio\n",
    "        whisper_transcription = whisper_model.transcribe(str(child_file), fp16=False)\n",
    "\n",
    "        # Store the transcription in the audio_metadata_dict_by_video_url\n",
    "        audio_metadata_dict_by_video_url[video_url] = whisper_transcription\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Error getting audio file path for video {video_url}: '{e}'\\nTraceback is as follows:\\n{traceback.format_exc()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Transcription to GBQ\n",
    "Now that I've transcribed all of these videos, I'm going to upload the transcriptions to GBQ. \n",
    "\n",
    "First, I'll transform the DataFrame to add some needed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the audio_metadata_dict_by_video_url\n",
    "audio_metadata_df = pd.DataFrame.from_dict(\n",
    "    audio_metadata_dict_by_video_url, orient=\"index\"\n",
    ")\n",
    "\n",
    "# Reset the index into a \"url\" column\n",
    "audio_metadata_df.reset_index(inplace=True, names=[\"url\"])\n",
    "\n",
    "# Explode the \"segments\" column\n",
    "audio_metadata_df = audio_metadata_df.explode(\"segments\")\n",
    "\n",
    "# Rename the \"segment\" column to \"segment\" in the audio_metadata_df\n",
    "audio_metadata_df = audio_metadata_df.rename(columns={\"segments\": \"segment\"})\n",
    "\n",
    "# Add a \"created_at\" column to the audio_metadata_df\n",
    "audio_metadata_df[\"created_at\"] = datetime.datetime.now()\n",
    "\n",
    "# Alter the \"text\" column so that it's extracted from the \"segment\" column\n",
    "audio_metadata_df[\"text\"] = audio_metadata_df[\"segment\"].apply(\n",
    "    lambda x: x.get(\"text\", None)\n",
    ")\n",
    "\n",
    "# Add a \"segment_type\" column to the audio_metadata_df\n",
    "audio_metadata_df[\"segment_type\"] = \"small_segment\"\n",
    "\n",
    "# We're going to extract some columns from the `segment` dictionary\n",
    "segment_columns_to_keep = [\"id\", \"seek\", \"start\", \"end\"]\n",
    "normalized_segments_df = pd.json_normalize(audio_metadata_df[\"segment\"])\n",
    "normalized_segments_df = normalized_segments_df[segment_columns_to_keep]\n",
    "\n",
    "# Rename all of the columns so that they have \"segment_\" prepended to them\n",
    "normalized_segments_df = normalized_segments_df.rename(\n",
    "    columns={col: f\"segment_{col}\" for col in normalized_segments_df.columns}\n",
    ")\n",
    "\n",
    "# Make the final_transcription_df\n",
    "final_transcription_df = pd.concat(\n",
    "    [\n",
    "        audio_metadata_df.drop(columns=[\"segment\"]).reset_index(drop=True),\n",
    "        normalized_segments_df.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I'll make a temporary table. This will help me ensure that I'm not re-uploading any transcriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the table we're going to create\n",
    "table_name = \"temp_transcriptions\"\n",
    "\n",
    "# Create the table\n",
    "gbq_utils.create_temporary_table_in_gbq(\n",
    "    dataframe=final_transcription_df,\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_name=GBQ_DATASET_ID,\n",
    "    table_name=table_name,\n",
    "    if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this temporary table in hand, I'm going to try and identify the videos whose transcriptions haven't been added yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following query will determine which transcripts we need to upload\n",
    "transcripts_to_upload_query = f\"\"\"\n",
    "SELECT\n",
    "  DISTINCT(temp_transcript.url)\n",
    "FROM\n",
    "  `backend_data.temp_transcriptions` temp_transcript\n",
    "LEFT JOIN\n",
    "  `backend_data.transcriptions` transcript\n",
    "ON\n",
    "  transcript.url = temp_transcript.url\n",
    "WHERE\n",
    "  transcript.created_at IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "transcripts_to_upload_df = pd.read_gbq(\n",
    "    transcripts_to_upload_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cross-referenced with the table, let's upload them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing the transcripts that we need to upload\n",
    "final_transcriptions_to_upload_df = final_transcription_df.merge(\n",
    "    transcripts_to_upload_df, on=\"url\"\n",
    ")\n",
    "\n",
    "# Use the gbq_utils to add rows to the `backend_data.transcriptions` table\n",
    "gbq_utils.add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"transcriptions\",\n",
    "    rows=final_transcription_df.to_dict(orient=\"records\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll delete the `temp_transactions` table and the `temp_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the temporary table\n",
    "gbq_utils.delete_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=table_name,\n",
    ")\n",
    "\n",
    "# Delete the temp_data directory and everything in it\n",
    "for child_file in Path(\"temp_data/\").iterdir():\n",
    "    child_file.unlink()\n",
    "Path(\"temp_data/\").rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching Video Metadata\n",
    "The next part of the pipeline involves enriching the video data. \n",
    "\n",
    "I'll start by determining which videos need their metadata enriched: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query below will define the videos that we need to enrich\n",
    "videos_to_enrich_query = f\"\"\"\n",
    "SELECT\n",
    "  metadata.id,\n",
    "  metadata.url,\n",
    "  metadata.title,\n",
    "  metadata.description\n",
    "FROM\n",
    "  `backend_data.video_metadata` metadata\n",
    "WHERE NOT EXISTS (\n",
    "  SELECT 1\n",
    "  FROM `backend_data.enriched_video_metadata` enriched_metadata\n",
    "  WHERE enriched_metadata.url = metadata.url\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "videos_to_enrich_df = pd.read_gbq(videos_to_enrich_query, project_id=GBQ_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to enrich the video metadata. Right now, this is a pretty simple exercise. There are only two new fields we're looking to add: \n",
    "\n",
    "- `video_type`\n",
    "- `review_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column containing the video type\n",
    "videos_to_enrich_df[\"video_type\"] = videos_to_enrich_df[\"title\"].apply(\n",
    "    lambda x: enrichment_utils.classify_video_type(x)\n",
    ")\n",
    "\n",
    "# Add a column containing the review score\n",
    "videos_to_enrich_df[\"review_score\"] = videos_to_enrich_df[\"description\"].apply(\n",
    "    lambda x: enrichment_utils.extract_review_score(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've enriched this video metadata, we can upload it to the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame that we're going to upload\n",
    "enriched_metadata_to_upload_df = videos_to_enrich_df.copy()\n",
    "\n",
    "# Drop the title and description columns\n",
    "enriched_metadata_to_upload_df.drop(columns=[\"title\", \"description\"], inplace=True)\n",
    "\n",
    "# Replace the NaN values with None\n",
    "enriched_metadata_to_upload_df[\"review_score\"] = enriched_metadata_to_upload_df[\n",
    "    \"review_score\"\n",
    "].replace({np.nan: None})\n",
    "\n",
    "# Add the rows to the `backend_data.enriched_video_metadata` table\n",
    "gbq_utils.add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"enriched_video_metadata\",\n",
    "    rows=enriched_metadata_to_upload_df.to_dict(orient=\"records\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Transcriptions\n",
    "Next up: we're going to embed some of the different audio transcriptions we've got. \n",
    "\n",
    "I'll start by determining which pieces of text we need to embed. This will involve checking the `transcriptions` table for video URLs that aren't represented within the `embeddings` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query below will determine which transcriptions we need to embed\n",
    "transcriptions_to_embed_query = f\"\"\"\n",
    "SELECT\n",
    "  transcript.*\n",
    "FROM\n",
    "  `backend_data.transcriptions` transcript\n",
    "LEFT JOIN\n",
    "  `backend_data.embeddings` embedding\n",
    "ON\n",
    "  embedding.video_url = transcript.url\n",
    "WHERE\n",
    "  embedding.id IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "transcriptions_to_embed_df = pd.read_gbq(\n",
    "    transcriptions_to_embed_query, project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: we need to create chunks of segments. There's a **whole** ton of literature on different chunking methods ([see here](https://www.pinecone.io/learn/chunking-strategies/)), but for right now, I'm going to basically ignore all of that in lieu of making some fixed-size chunks. \n",
    "\n",
    "I can always come back later to re-evaluate the chunking situation. For now, I'm going to try and embed both 4-segment and 8-segment chunks. In the future, I'll revisit this to try and make smarter decisions about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the segment chunk sizes\n",
    "segment_chunk_sizes = [4, 8]\n",
    "\n",
    "# Initialize a list to hold the segment_chunk rows\n",
    "segment_chunks = []\n",
    "\n",
    "# Iterate through each unique URL in the DataFrame\n",
    "for url in transcriptions_to_embed_df[\"url\"].unique():\n",
    "    # Filter the DataFrame for the current URL\n",
    "    url_df = transcriptions_to_embed_df[transcriptions_to_embed_df[\"url\"] == url]\n",
    "\n",
    "    # Extract the video ID from the URL\n",
    "    video_id = url.split(\"watch?v=\")[-1]\n",
    "\n",
    "    # Sort the DataFrame by the segment_start\n",
    "    url_df = url_df.sort_values(by=[\"segment_start\"])\n",
    "\n",
    "    # Iterate through the defined chunk sizes\n",
    "    for chunk_size in segment_chunk_sizes:\n",
    "        # Iterate through the segments in steps of chunk_size\n",
    "        for i in range(0, len(url_df), chunk_size):\n",
    "            # Get the chunk of segments\n",
    "            segment_chunk_df = url_df.iloc[i : i + chunk_size]\n",
    "\n",
    "            # Concatenate the text of the segments to form the chunk text\n",
    "            chunk_text = \" \".join(segment_chunk_df[\"text\"].tolist())\n",
    "\n",
    "            # Determine a new ID for this particular segment chunk\n",
    "            segment_chunk_id = f\"{video_id}_{i}_{i+chunk_size}\"\n",
    "\n",
    "            # Create a dictionary for the segment_chunk row\n",
    "            segment_chunk_row = {\n",
    "                \"id\": segment_chunk_id,\n",
    "                \"video_url\": video_id,\n",
    "                \"embedding_type\": \"segment_chunk\",\n",
    "                \"start_segment\": i,\n",
    "                \"end_segment\": i + chunk_size,\n",
    "                \"segment_length\": chunk_size,\n",
    "                \"text\": chunk_text.strip(),\n",
    "            }\n",
    "\n",
    "            # Append the segment_chunk_row to the list of segment_chunks\n",
    "            segment_chunks.append(segment_chunk_row)\n",
    "\n",
    "# Create a DataFrame that has the segment_chunks\n",
    "segment_chunks_df = pd.DataFrame(segment_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of these chunks, we're going to embed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:01<00:00, 78.87it/s] \n"
     ]
    }
   ],
   "source": [
    "# We're going to collect all of the embeddings in this list\n",
    "embeddings_col = openai_utils.embed_text_list(list(segment_chunks_df[\"text\"]))\n",
    "\n",
    "# Add this column containing all of the embeddings to the segment_chunks_df\n",
    "segment_chunks_df[\"embedding\"] = embeddings_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got all of the embeddings, I'll upload them to GCS. I'll check if the bucket exists first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket neural-needledrop.neural-needledrop-embeddings created\n"
     ]
    }
   ],
   "source": [
    "# Create the embeddings bucket if it doesn't exist\n",
    "gcs_utils.create_bucket(\n",
    "    bucket_name=\"neural-needledrop-embeddings\", project_id=GBQ_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll try to save all of the embeddings as temporary `.npy` files. This will help me upload them to GCS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the temp_embeddings folder if it doesn't exist\n",
    "temp_folder_name = \"temp_embeddings\"\n",
    "Path(temp_folder_name).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# We're going to keep a list of the file paths of the embeddings we save\n",
    "embedding_file_paths = []\n",
    "for row in segment_chunks_df.itertuples():\n",
    "    video_id = row.id\n",
    "    emb_filename = f\"{temp_folder_name}/{video_id}.npy\"\n",
    "    emb = row.embedding\n",
    "    openai_utils.save_as_npy(\n",
    "        embedding=emb,\n",
    "        file_name=emb_filename,\n",
    "    )\n",
    "    embedding_file_paths.append(emb_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to upload all of the `.npy` files to the GCS bucket, and then delete the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a GCS client\n",
    "gcs_client = storage.Client(project=GBQ_PROJECT_ID)\n",
    "\n",
    "for file_path in tqdm(embedding_file_paths, disable=not tqdm_enabled):\n",
    "    gcs_utils.upload_file_to_bucket(\n",
    "        file_path=file_path,\n",
    "        bucket_name=\"neural-needledrop-embeddings\",\n",
    "        project_id=GBQ_PROJECT_ID,\n",
    "        gcs_client=gcs_client\n",
    "    )\n",
    "\n",
    "    # Remove the file\n",
    "    Path(file_path).unlink()\n",
    "\n",
    "# Delete the temp_embeddings folder\n",
    "Path(temp_folder_name).rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally: now that we've uploaded all of the embeddings to GCS, we can add some rows to the `embeddings` table that contain information about these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 139 row(s) into backend_data:embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Define the rows_to_upload list\n",
    "rows_to_upload = []\n",
    "\n",
    "# Iterate through the segment_chunks_df and extract the rows\n",
    "for row in segment_chunks_df.itertuples():\n",
    "    # Extract the row\n",
    "    row_dict = {\n",
    "        \"id\": row.id,\n",
    "        \"video_url\": row.video_url,\n",
    "        \"gcs_uri\": f\"gs://neural-needledrop-embeddings/{row.id}.npy\",\n",
    "        \"embedding_type\": row.embedding_type,\n",
    "        \"start_segment\": row.start_segment,\n",
    "        \"end_segment\": row.end_segment,\n",
    "        \"segment_length\": row.segment_length,\n",
    "    }\n",
    "\n",
    "    # Append the row to the rows_to_upload\n",
    "    rows_to_upload.append(row_dict)\n",
    "\n",
    "# Add the rows to the `backend_data.embeddings` table\n",
    "gbq_utils.add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"embeddings\",\n",
    "    rows=rows_to_upload,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
