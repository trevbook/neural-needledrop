{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finalizing Pipeline**\n",
    "In **`02. Prototyping Data Pipeline`**, I scoped out the entire data pipeline. Once I knew that it was running properly, I wanted to make it more configurable and contained. \n",
    "\n",
    "This notebook is going to be similar to that notebook, but will invoke entire configurable pipeline methods instead of multiple-cell stretches for each section. \n",
    "\n",
    "Each of the methods invoked will correspond with a step of the pipeline. There are a couple of different ones: \n",
    "\n",
    "- **Initialize Cloud Resources:** This will make sure that all of the GBQ tables & GCS buckets exist. It'll have an optional attribute for deleting *everything*. \n",
    "\n",
    "- **Download Video Metadata:** Next up: downloading some video metadata. This will identify which videos that a user needs to find, and then uses `pytube` to download some metadata. \n",
    "\n",
    "- **Enrich Video Metadata:** This step will determine what type of video each video is (album review, weekly track roundup, etc.), and extract review scores from the description\n",
    "\n",
    "- **Downloading Video Audio:** This step will download the audio of videos we haven't downloaded yet\n",
    "\n",
    "- **Transcribing Audio:** Next: this step uses OpenAI's Whisper model to transcribe all of the audio we've downloaded. \n",
    "\n",
    "- **Embedding Transcriptions:** Finally, we're going to embed some of the transcriptions that we've created using Whisper. \n",
    "\n",
    "Each of these methods shares a couple of key design steps: \n",
    "\n",
    "- Idempotency: The methods can be retried, and won't necessarily overwrite things\n",
    "- Logging: Each of the methods logs information (using various logging levels) to Google Cloud Logging \n",
    "- Configurable: Different parameters of the pipeline step can be indicated through command line arguments. I've also got a way to load in these configurations via .yml files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\pipeline\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOG_TO_CONSOLE=True\n",
      "env: LOG_LEVEL=INFO\n",
      "env: TQDM_ENABLED=True\n"
     ]
    }
   ],
   "source": [
    "# Set up some environment variables to configure the logging \n",
    "%env LOG_TO_CONSOLE=True\n",
    "%env LOG_LEVEL=INFO\n",
    "%env TQDM_ENABLED=True\n",
    "\n",
    "# General import statements\n",
    "import pandas as pd\n",
    "\n",
    "# Import each of the different jobs\n",
    "from jobs.initialize_cloud_resources import run_initialize_cloud_resources_job\n",
    "from jobs.download_video_metadata import run_download_video_metadata_job\n",
    "from jobs.enrich_video_metadata import run_enrich_video_metadata_job\n",
    "from jobs.download_audio import run_download_audio_job\n",
    "from jobs.transcribe_audio import run_transcribe_audio_job\n",
    "from jobs.embed_transcriptions import run_embed_transcriptions_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs\n",
    "Below, I'm going to run each of the individual jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Cloud Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:28:15,511 - pipeline.initialize_resources - INFO - Starting the INITIALIZE RESOURCES job.\n",
      "2024-01-31 22:28:24,917 - pipeline.initialize_resources - INFO - Finished the INITIALIZE RESOURCES job.\n"
     ]
    }
   ],
   "source": [
    "# Run the initialize resources job\n",
    "run_initialize_cloud_resources_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Video Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:28:25,968 - pipeline.download_video_metadata - INFO - Determining whether or not to scrape this channel.\n",
      "2024-01-31 22:28:29,702 - pipeline.download_video_metadata - INFO - Starting the DOWNLOAD VIDEO METADATA job.\n",
      "2024-01-31 22:28:29,703 - pipeline.download_video_metadata - INFO - Crawling channel https://www.youtube.com/c/theneedledrop.\n",
      "2024-01-31 22:28:31,895 - pipeline.download_video_metadata - INFO - Most recent video url found: https://www.youtube.com/watch?v=AXeLuZ9SUSM\n",
      "2024-01-31 22:28:42,455 - pipeline.download_video_metadata - INFO - Found 1 videos to parse for channel https://www.youtube.com/c/theneedledrop.\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.59s/it]\n",
      "2024-01-31 22:28:47,054 - pipeline.download_video_metadata - INFO - Finished parsing metadata for 1 videos.\n",
      "2024-01-31 22:28:47,971 - pipeline.download_video_metadata - INFO - Finished adding rows to the `video_metadata` table.\n",
      "2024-01-31 22:28:47,972 - pipeline.download_video_metadata - INFO - Finished the DOWNLOAD VIDEO METADATA job.\n"
     ]
    }
   ],
   "source": [
    "# Run the download video metadata job\n",
    "run_download_video_metadata_job(\n",
    "    channel_url=\"https://www.youtube.com/c/theneedledrop\",\n",
    "    video_limit=10,\n",
    "    stop_at_most_recent_video=True,\n",
    "    video_parse_step_size=25,\n",
    "    time_to_sleep_between_requests=2,\n",
    "    sleep_time_multiplier=2.25,\n",
    "    n_days_to_not_scrape=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Video Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:28:48,073 - pipeline.enrich_video_metadata - INFO - Starting the ENRICH VIDEO METADATA job.\n",
      "2024-01-31 22:28:50,137 - pipeline.enrich_video_metadata - INFO - Found 1 videos to enrich\n",
      "2024-01-31 22:28:50,138 - pipeline.enrich_video_metadata - INFO - Adding `video_type` enrichment.\n",
      "2024-01-31 22:28:50,140 - pipeline.enrich_video_metadata - INFO - Adding `review_score` enrichment.\n",
      "2024-01-31 22:28:53,473 - pipeline.enrich_video_metadata - INFO - Finished the ENRICH VIDEO METADATA job.\n"
     ]
    }
   ],
   "source": [
    "# Run the enrich video metadata job\n",
    "run_enrich_video_metadata_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:28:53,572 - pipeline.download_audio - INFO - Starting the DOWNLOAD AUDIO job.\n",
      "2024-01-31 22:28:58,550 - pipeline.download_audio - INFO - Downloading audio for 1 videos.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2024-01-31 22:29:09,867 - pipeline.download_audio - ERROR - Error downloading audio for https://www.youtube.com/watch?v=S69_SmKkBIY: 'IncompleteRead(2058102 bytes read, 426611 more expected)'\n",
      "The traceback is:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\pipeline\\jobs\\download_audio.py\", line 112, in run_download_audio_job\n",
      "    youtube_utils.download_audio_from_video(\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\pipeline\\utils\\youtube.py\", line 109, in download_audio_from_video\n",
      "    raise e\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\pipeline\\utils\\youtube.py\", line 88, in download_audio_from_video\n",
      "    for stream in video.streams.filter(only_audio=True):\n",
      "                  ^^^^^^^^^^^^^\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\.venv\\Lib\\site-packages\\pytubefix\\__main__.py\", line 321, in streams\n",
      "    return StreamQuery(self.fmt_streams)\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\.venv\\Lib\\site-packages\\pytubefix\\__main__.py\", line 206, in fmt_streams\n",
      "    extract.apply_signature(stream_manifest, self.vid_info, self.js)\n",
      "                                                            ^^^^^^^\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\.venv\\Lib\\site-packages\\pytubefix\\__main__.py\", line 164, in js\n",
      "    self._js = request.get(self.js_url)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\data\\programming\\neural-needledrop\\.venv\\Lib\\site-packages\\pytubefix\\request.py\", line 54, in get\n",
      "    return response.read().decode(\"utf-8\")\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\http\\client.py\", line 481, in read\n",
      "    s = self._safe_read(self.length)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\http\\client.py\", line 632, in _safe_read\n",
      "    raise IncompleteRead(data, amt-len(data))\n",
      "http.client.IncompleteRead: IncompleteRead(2058102 bytes read, 426611 more expected)\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.32s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2024-01-31 22:29:09,873 - pipeline.download_audio - WARNING - When trying to upload audio for https://www.youtube.com/watch?v=S69_SmKkBIY, we couldn't find the audio file at temp_audio_data\\S69_SmKkBIY.m4a. Skipping...\n",
      "100%|██████████| 1/1 [00:00<00:00, 785.89it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'temp_audio_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the run_download_audio_job\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_download_audio_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_max_videos_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\data\\programming\\neural-needledrop\\pipeline\\jobs\\download_audio.py:236\u001b[0m, in \u001b[0;36mrun_download_audio_job\u001b[1;34m(n_max_videos_to_download, time_to_sleep_between_requests, sleep_multiplier, temp_download_directory, gbq_client, gcs_client)\u001b[0m\n\u001b[0;32m    231\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhile uploading audio for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, we ran into an error: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe traceback is:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Make sure that all of the files have been deleted\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtemp_download_directory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\pathlib.py:931\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    928\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_child_relpath(name)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'temp_audio_data'"
     ]
    }
   ],
   "source": [
    "# Run the run_download_audio_job\n",
    "run_download_audio_job(\n",
    "    n_max_videos_to_download=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the transcribe_audio job\n",
    "run_transcribe_audio_job(\n",
    "    n_max_to_transcribe=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:29:29,938 - pipeline.embed_transcriptions - INFO - Found 773 individual transcription segments to embed.\n",
      "2024-01-31 22:29:29,958 - pipeline.embed_transcriptions - INFO - Groupped the transcription segments into 261 segment chunks.\n",
      "2024-01-31 22:29:29,959 - pipeline.embed_transcriptions - INFO - Embedding the transcription chunks...\n",
      "Embedding Texts: 100%|██████████| 261/261 [00:37<00:00,  6.98it/s]\n",
      "2024-01-31 22:30:07,386 - pipeline.embed_transcriptions - INFO - Finished embedding the transcription chunks.\n",
      "2024-01-31 22:30:07,388 - pipeline.embed_transcriptions - INFO - Storing the embeddings in GCS...\n",
      "100%|██████████| 261/261 [00:11<00:00, 22.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the embed_transcriptions job\n",
    "run_embed_transcriptions_job(max_videos_to_embed=10, max_parallel_embedding_workers=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
