{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uploading Previously Scraped Data**\n",
    "When [I first made my prototype for Neural Needledrop](https://github.com/trevbook/neural-needle-drop-archive), I saved all of the `.mp3` files locally. Instead of re-scraping them all, I can just upload *them* to my cloud database - that way, I'll jumpstart all of my data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\pipeline\n",
      "env: LOG_TO_CONSOLE=True\n",
      "env: LOG_LEVEL=INFO\n",
      "env: TQDM_ENABLED=True\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up some envvars\n",
    "%env LOG_TO_CONSOLE=True\n",
    "%env LOG_LEVEL=INFO\n",
    "%env TQDM_ENABLED=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from pandas_gbq import read_gbq\n",
    "\n",
    "# Importing custom modules\n",
    "from utils.logging import get_logger\n",
    "from utils.gbq import add_rows_to_table, delete_table, create_table\n",
    "from utils.gcs import upload_files_to_bucket, list_bucket_objects\n",
    "from utils.settings import GBQ_PROJECT_ID, GBQ_DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "Below, I'm going to load in all of the data that I've got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3974/3974 [01:00<00:00, 65.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the folder that contains the data\n",
    "archive_data_folder = Path(\n",
    "    \"D:/data/programming/neural-needle-drop-archive/data/theneedledrop_scraping\"\n",
    ")\n",
    "\n",
    "# Iterate through each of the files in the folder and store some information\n",
    "archive_data_df_records = []\n",
    "for child in tqdm(list(archive_data_folder.iterdir())):\n",
    "    # If the child is not a directory itself, continue\n",
    "    if not child.is_dir():\n",
    "        continue\n",
    "\n",
    "    # Extract the video ID from the folder name\n",
    "    video_id = child.name\n",
    "    video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "    # Identify any files within `child` that have the `.mp3` extension\n",
    "    try:\n",
    "        audio_file = list(child.glob(\"*.mp3\"))[0]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Rename the audio file to the video ID\n",
    "    audio_file.rename(child / f\"{video_id}.mp3\")\n",
    "\n",
    "    # Load in the transcription JSON file\n",
    "    transcription_path = child / \"transcription.json\"\n",
    "    if not transcription_path.exists():\n",
    "        continue\n",
    "    with open(transcription_path) as f:\n",
    "        transcription_dict = json.load(f)\n",
    "        segments = transcription_dict.get(\"segments\", [])\n",
    "    if len(segments) == 0:\n",
    "        continue\n",
    "\n",
    "    # Load in the details JSON file\n",
    "    details_path = child / \"details.json\"\n",
    "    if not details_path.exists():\n",
    "        continue\n",
    "    with open(details_path) as f:\n",
    "        details_dict = json.load(f)\n",
    "\n",
    "    # Convert the \"created at\" timestamp from float to a datetime object\n",
    "    created_at = datetime.datetime.fromtimestamp(child.stat().st_ctime)\n",
    "\n",
    "    # Create a \"transcription_data\" list\n",
    "    transcription_data = [\n",
    "        {\n",
    "            \"url\": video_url,\n",
    "            \"text\": segment_info.get(\"text\", None),\n",
    "            \"language\": \"en\",\n",
    "            \"created_at\": created_at,\n",
    "            \"segment_type\": \"small_segment\",\n",
    "            \"segment_id\": segment_info.get(\"id\", None),\n",
    "            \"segment_seek\": segment_info.get(\"seek\", None),\n",
    "            \"segment_start\": segment_info.get(\"start\", None),\n",
    "            \"segment_end\": segment_info.get(\"end\", None),\n",
    "        }\n",
    "        for segment_info in segments\n",
    "    ]\n",
    "\n",
    "    # Store the data in a dataframe\n",
    "    archive_data_df_records.append(\n",
    "        {\n",
    "            \"video_id\": video_id,\n",
    "            \"video_url\": video_url,\n",
    "            \"created_at\": created_at,\n",
    "            \"transcription_data\": transcription_data,\n",
    "            \"metadata\": details_dict,\n",
    "            \"audio_path\": child / f\"{video_id}.mp3\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Make a dataframe from the records\n",
    "archive_data_df = pd.DataFrame(archive_data_df_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Audio to GCS\n",
    "I'll start by uploading all of the `.mp3` files into GCS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which files are currently uploaded to the bucket\n",
    "cur_files_uploaded = list_bucket_objects(\n",
    "    bucket_name=\"neural-needledrop-audio\", project_id=GBQ_PROJECT_ID\n",
    ")\n",
    "video_ids_uploaded = [file.split(\".mp3\")[0] for file in cur_files_uploaded]\n",
    "\n",
    "# Determine which files have not yet been uploaded\n",
    "videos_to_upload_df = archive_data_df[\n",
    "    ~archive_data_df[\"video_id\"].isin(video_ids_uploaded)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've determined the files to upload, I'll upload them to GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all of the data to GCS\n",
    "upload_files_to_bucket(\n",
    "    file_path_list=list(videos_to_upload_df[\"audio_path\"]),\n",
    "    bucket_name=\"neural-needledrop-audio\",\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    max_workers=1,\n",
    "    show_progress=True,\n",
    "    logger=get_logger(\"upload_files_to_bucket\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Transcripts to GBQ \n",
    "Another thing I'm interested in: uploading the spare transcripts that I have to GBQ. I'll start by determining which transcripts I need to upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "# This query will determine which videos don't have transcripts\n",
    "videos_needing_transcripts_query = \"\"\"\n",
    "SELECT\n",
    "  DISTINCT(video.url) AS video_url\n",
    "FROM\n",
    "  `neural-needledrop.backend_data.video_metadata` video\n",
    "LEFT JOIN\n",
    "  `neural-needledrop.backend_data.transcriptions` transcription\n",
    "ON\n",
    "  video.url = transcription.url\n",
    "WHERE\n",
    "  transcription.text IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query \n",
    "videos_needing_transcripts_df = read_gbq(videos_needing_transcripts_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've figured out which transcriptions I need, I'll check my archive data to see if I've got any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,341 videos whose transcriptions I need to upload\n"
     ]
    }
   ],
   "source": [
    "# Identify the transcripts that I need to upload\n",
    "transcriptions_to_upload_df = archive_data_df.dropna(subset=[\"transcription_data\"]).merge(\n",
    "    videos_needing_transcripts_df, on=\"video_url\"\n",
    ").copy()\n",
    "\n",
    "# Print some information\n",
    "print(f\"There are {len(transcriptions_to_upload_df):,} videos whose transcriptions I need to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll need to transform all of the data to upload it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to collect rows that we want to add to the table\n",
    "transcriptions_rows = []\n",
    "for row in transcriptions_to_upload_df.itertuples():\n",
    "    cur_transcription_data = row.transcription_data\n",
    "    transcriptions_rows += cur_transcription_data\n",
    "\n",
    "# Next, use the GBQ util add_rows_to_table to add the rows to the table\n",
    "add_rows_to_table(\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    dataset_id=GBQ_DATASET_ID,\n",
    "    table_id=\"transcriptions\",\n",
    "    rows=transcriptions_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing GBQ Tables\n",
    "Next, we're going to need to edit the necessary GBQ tables in order to include new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Metadata\n",
    "First, I'm going to update the video metadata table. I'll start by downloading the current table in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the entire video metadata table\n",
    "current_audio_files_df = pd.read_gbq(\n",
    "    f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.audio`\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to merge together old and new information to create a new table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which files are not in the table\n",
    "new_audio_files_df = (\n",
    "    archive_data_df[\n",
    "        ~archive_data_df[\"video_url\"].isin(current_audio_files_df[\"video_url\"].unique())\n",
    "    ]\n",
    "    .copy()[[\"video_url\", \"audio_path\", \"created_at\"]]\n",
    "    .rename(columns={\"created_at\": \"scrape_date\", \"audio_path\": \"audio_gcr_uri\"})\n",
    ")\n",
    "\n",
    "# Edit the columns to match the audio table\n",
    "new_audio_files_df[\"audio_gcr_uri\"] = new_audio_files_df[\"audio_gcr_uri\"].apply(\n",
    "    lambda x: f\"gs://neural-needledrop-audio/{Path(x).name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to just totally replace the table with all of the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both to the same datetime type before merging\n",
    "# current_audio_files_df[\"scrape_date\"] = current_audio_files_df[\"scrape_date\"]\n",
    "\n",
    "new_audio_files_df[\"scrape_date\"] = new_audio_files_df[\"scrape_date\"].astype(\n",
    "    \"datetime64[ns]\"\n",
    ")\n",
    "\n",
    "# Now you can merge\n",
    "merged_audio_files_df = pd.concat([current_audio_files_df, new_audio_files_df], axis=0)\n",
    "\n",
    "# Drop any duplicates\n",
    "merged_audio_files_df = merged_audio_files_df.drop_duplicates(\n",
    "    subset=[\"video_url\"], keep=\"last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_audio_files_df[\"scrape_date\"] = merged_audio_files_df[\"scrape_date\"].apply(\n",
    "    lambda x: x.replace(tzinfo=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now upload this table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new table\n",
    "merged_audio_files_df.to_gbq(\n",
    "    destination_table=f\"{GBQ_DATASET_ID}.audio\",\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the `audio` Table\n",
    "I've realized that there are items in the `audio` table that aren't actually downloaded. I need to fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what's in the current audio table\n",
    "full_audio_table_df = pd.read_gbq(\"SELECT * FROM `neural-needledrop.backend_data.audio`\")\n",
    "\n",
    "# Filter out all of the rows whose audio files are not in the bucket\n",
    "correct_audio_table_df = full_audio_table_df[\n",
    "    full_audio_table_df[\"audio_gcr_uri\"].isin(\n",
    "        [\n",
    "            f\"gs://neural-needledrop-audio/{file_name}\"\n",
    "            for file_name in cur_files_uploaded\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Upload this DataFrame to GBQ\n",
    "correct_audio_table_df.to_gbq(\n",
    "    destination_table=f\"{GBQ_DATASET_ID}.audio\",\n",
    "    project_id=GBQ_PROJECT_ID,\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
