{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functionalizing the Database Update**\n",
    "Now that I've got the database auto-deploying via CI/CD, I need to write some sort of script to automatically update things. I'm going to prototype that method here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needledrop\\database\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "from pandas_gbq import read_gbq\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, MetaData, Column, Integer, String, DateTime\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.sql import text\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing modules custom-built for this project\n",
    "from utils.settings import (\n",
    "    POSTGRES_USER,\n",
    "    POSTGRES_PASSWORD,\n",
    "    POSTGRES_HOST,\n",
    "    POSTGRES_PORT,\n",
    "    POSTGRES_DB,\n",
    "    LOG_TO_CONSOLE,\n",
    "    GBQ_PROJECT_ID,\n",
    "    GBQ_DATASET_ID\n",
    ")\n",
    "from utils.logging import get_logger\n",
    "from utils.postgres import delete_table, create_table\n",
    "from utils.gcs import download_file_from_bucket\n",
    "from utils.postgres import query_postgres, upload_to_table, delete_table\n",
    "\n",
    "# Set up a logger for this notebook\n",
    "logger = get_logger(\"postgres_notebook\", log_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also set up the Postgres database connector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string to the database\n",
    "postgres_connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(postgres_connection_string)\n",
    "metadata = MetaData()\n",
    "session = sessionmaker(bind=engine)()\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterizing Function\n",
    "Below, I'm going to set up different arguments for the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the folder where the data is stored\n",
    "database_folder = \"data\"\n",
    "\n",
    "# Indicate whether we want to re-create tables if they already exist\n",
    "recreate_tables = False\n",
    "\n",
    "# Indicate the chunksize for the Postgres upload\n",
    "postgres_upload_chunksize = 5000\n",
    "\n",
    "# Indicate a couple of settings re: index creation \n",
    "postgres_maintenance_work_mem = \"6GB\"\n",
    "postgres_max_parallel_maintenance_workers = 7\n",
    "\n",
    "# Indicate a limit for the number of videos whose embeddings we want to upload\n",
    "max_n_videos_to_update_embeddings = 1000\n",
    "\n",
    "# Indicate some settings related to the embeddings index\n",
    "recreate_embeddings_index_if_exists = True\n",
    "embeddings_index_ivfflat_nlist = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Function\n",
    "Below, I've got the entire function, runnable via different cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Table Deletion\n",
    "If the user wanted to delete any tables, then we'll do it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# OPTIONAL TABLE DELETION\n",
    "# =======================\n",
    "# If the user wants to delete the tables, then do so\n",
    "\n",
    "if recreate_tables:\n",
    "    # Log that we're deleting the tables\n",
    "    logger.info(\"DELETING EACH OF THE POSTGRES TABLES...\")\n",
    "    tables_to_delete = [\"video_metadata\", \"embeddings\", \"transcriptions\"]\n",
    "    for table in tables_to_delete:\n",
    "        delete_table(table, engine, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Initialization Check\n",
    "Below, we'll try and create the tables if they don't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:07:51,765 - postgres_notebook - INFO - CREATING THE video_metadata TABLE...\n",
      "2024-02-18 23:07:51,765 - postgres_notebook - ERROR - Error creating table 'video_metadata': Table 'video_metadata' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "2024-02-18 23:07:51,769 - postgres_notebook - ERROR - An error occurred while creating the video_metadata table: Table 'video_metadata' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "2024-02-18 23:07:51,770 - postgres_notebook - INFO - CREATING THE transcriptions TABLE...\n",
      "2024-02-18 23:07:51,770 - postgres_notebook - ERROR - Error creating table 'transcriptions': Table 'transcriptions' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "2024-02-18 23:07:51,771 - postgres_notebook - ERROR - An error occurred while creating the transcriptions table: Table 'transcriptions' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "2024-02-18 23:07:51,771 - postgres_notebook - INFO - CREATING THE embeddings TABLE...\n",
      "2024-02-18 23:07:51,775 - postgres_notebook - ERROR - Error creating table 'embeddings': Table 'embeddings' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "2024-02-18 23:07:51,776 - postgres_notebook - ERROR - An error occurred while creating the embeddings table: Table 'embeddings' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# VIDEO_METADATA INITIALIZATION\n",
    "# =============================\n",
    "# Below, we'll initialize the video_metadata table in the database\n",
    "\n",
    "# Log that we're creating the video_metadata table\n",
    "logger.info(\"CREATING THE video_metadata TABLE...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    # Define the schema that we'll be using for this table\n",
    "    schema = [\n",
    "        Column(\"id\", String, primary_key=True),\n",
    "        Column(\"title\", String),\n",
    "        Column(\"length\", Integer),\n",
    "        Column(\"channel_id\", String),\n",
    "        Column(\"channel_name\", String),\n",
    "        Column(\"short_description\", String),\n",
    "        Column(\"description\", String),\n",
    "        Column(\"view_ct\", Integer),\n",
    "        Column(\"url\", String),\n",
    "        Column(\"small_thumbnail_url\", String),\n",
    "        Column(\"large_thumbnail_url\", String),\n",
    "        Column(\"video_type\", String),\n",
    "        Column(\"review_score\", Integer),\n",
    "        Column(\"publish_date\", DateTime),\n",
    "        Column(\"scrape_date\", DateTime),\n",
    "    ]\n",
    "\n",
    "    # Create the table\n",
    "    create_table(\"video_metadata\", schema, engine, metadata, logger)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    # Log the error\n",
    "    logger.error(f\"An error occurred while creating the video_metadata table: {e}\")\n",
    "\n",
    "# =============================\n",
    "# TRANSCRIPTIONS INITIALIZATION\n",
    "# =============================\n",
    "# Below, we'll initialize the transcriptions table in the database\n",
    "\n",
    "# Log that\n",
    "logger.info(\"CREATING THE transcriptions TABLE...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    # Define the schema that we'll be using for this table\n",
    "    transcriptions_table_schema = [\n",
    "        Column(\"url\", String),\n",
    "        Column(\"text\", String),\n",
    "        Column(\"segment_id\", Integer),\n",
    "        Column(\"segment_seek\", Integer),\n",
    "        Column(\"segment_start\", Integer),\n",
    "        Column(\"segment_end\", Integer),\n",
    "    ]\n",
    "\n",
    "    # Create the table\n",
    "    create_table(\n",
    "        \"transcriptions\", transcriptions_table_schema, engine, metadata, logger\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    # Log the error\n",
    "    logger.error(f\"An error occurred while creating the transcriptions table: {e}\")\n",
    "\n",
    "# =========================\n",
    "# EMBEDDINGS INITIALIZATION\n",
    "# =========================\n",
    "# Below, we'll initialize the embeddings table in the database\n",
    "\n",
    "# Log that\n",
    "logger.info(\"CREATING THE embeddings TABLE...\")\n",
    "\n",
    "try:\n",
    "    # Enable the pgvector Extension\n",
    "    session.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))\n",
    "    session.commit()\n",
    "\n",
    "    # Now, we're going to create a table for the embeddings\n",
    "    embeddings_table_schema = [\n",
    "        Column(\"id\", String, primary_key=True),\n",
    "        Column(\"url\", String),\n",
    "        Column(\"embedding_type\", String),\n",
    "        Column(\"start_segment\", Integer),\n",
    "        Column(\"end_segment\", Integer),\n",
    "        Column(\"segment_length\", Integer),\n",
    "        Column(\"embedding\", Vector(1536)),\n",
    "    ]\n",
    "\n",
    "    # Now, we're going to create a table for the embeddings\n",
    "    create_table(\"embeddings\", embeddings_table_schema, engine, metadata, logger)\n",
    "\n",
    "except Exception as e:\n",
    "    # Log the error\n",
    "    logger.error(f\"An error occurred while creating the embeddings table: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Data\n",
    "Now, we'll update all the tables using the freshest GBQ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:08:17,684 - postgres_notebook - INFO - CREATING TEMPORARY GBQ TABLES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 703.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# CREATING TEMPORARY GBQ TABLES\n",
    "# =============================\n",
    "# Below, I'm going to create temporary tables in GBQ that will allow me to compare the current state of the Postgres database with the state of the GBQ database. \n",
    "# This will allow me to determine which videos, transcriptions, and embeddings are currently in the Postgres database, but not in the GBQ database.\n",
    "\n",
    "# Log that we're creating the temporary GBQ tables\n",
    "logger.info(\"CREATING TEMPORARY GBQ TABLES...\")\n",
    "\n",
    "# Determine the videos currently in the `video_metadata` table\n",
    "cur_database_video_metadata_df = query_postgres(\n",
    "    \"SELECT id FROM video_metadata\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Determine the transcriptions currently in the `transcriptions` table\n",
    "cur_database_transcriptions_df = query_postgres(\n",
    "    \"SELECT DISTINCT(url) FROM transcriptions\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Determine the embeddings currently in the `embeddings` table\n",
    "cur_database_embeddings_df = query_postgres(\n",
    "    \"SELECT DISTINCT(id) FROM embeddings\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Upload the `cur_database_video_metadata_df` dataframe to a temporary table in GBQ\n",
    "cur_database_video_metadata_df.to_gbq(\n",
    "    f\"{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_video_metadata\",\n",
    "    if_exists=\"replace\",\n",
    ")\n",
    "\n",
    "# Upload the `cur_database_transcriptions_df` dataframe to a temporary table in GBQ\n",
    "cur_database_transcriptions_df.to_gbq(\n",
    "    f\"{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_transcriptions\",\n",
    "    if_exists=\"replace\",\n",
    ")\n",
    "\n",
    "# Upload the `cur_database_embeddings_df` dataframe to a temporary table in GBQ\n",
    "cur_database_embeddings_df.to_gbq(\n",
    "    f\"{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_embeddings\",\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`video_metadata`**\n",
    "The first table is the `video_metadata` table. I'll download that (and some columns from the `enriched_video_metadata` table) for all of the videos that I've transcribed and embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:09:00,077 - postgres_notebook - INFO - DETERMINING DELTA FOR video_metadata...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# DETERMINING DELTA FOR video_metadata\n",
    "# ====================================\n",
    "# Below, I'm going to determine the delta between the current state of the Postgres database and the state of the GBQ database.\n",
    "\n",
    "# Log that we're determining the delta for the video_metadata table\n",
    "logger.info(\"DETERMINING DELTA FOR video_metadata...\")\n",
    "\n",
    "# Define a query that'll grab all of the video metadata from the GBQ database\n",
    "video_metadata_query = f\"\"\"\n",
    "-- This query will select metadata for all of the videos that have transcriptions & embeddings\n",
    "SELECT\n",
    "  video.*,\n",
    "  enriched_video.video_type,\n",
    "  enriched_video.review_score\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.video_metadata` video\n",
    "JOIN\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.enriched_video_metadata` enriched_video\n",
    "ON\n",
    "  video.id = enriched_video.id\n",
    "WHERE\n",
    "  video.url IN (SELECT DISTINCT(url) FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions`)\n",
    "  AND\n",
    "  video.url IN (SELECT DISTINCT(video_url) AS url FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings`)\n",
    "  AND\n",
    "  video.id NOT IN (SELECT id FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_video_metadata`)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "video_metadata_df = read_gbq(video_metadata_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`transcriptions`**\n",
    "Next, I'm going to download all of the transcriptions for the videos I'd identified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:09:13,637 - postgres_notebook - INFO - DETERMINING DELTA FOR transcriptions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# DETERMINING DELTA FOR transcriptions\n",
    "# ====================================\n",
    "# Below, I'm going to determine the delta between the current state of the Postgres database and the state of the GBQ database.\n",
    "\n",
    "# Log that we're determining the delta for the transcriptions table\n",
    "logger.info(\"DETERMINING DELTA FOR transcriptions...\")\n",
    "\n",
    "# Declare the query that will download all of the relevant rows from the \n",
    "# transcription table\n",
    "transcriptions_query = f\"\"\"\n",
    "SELECT \n",
    "  transcription.url,\n",
    "  transcription.text,\n",
    "  transcription.segment_id,\n",
    "  transcription.segment_seek,\n",
    "  transcription.segment_start,\n",
    "  transcription.segment_end\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions` transcription\n",
    "JOIN (\n",
    "  SELECT\n",
    "    video.url\n",
    "  FROM\n",
    "    `neural-needledrop.backend_data.video_metadata` video\n",
    "  WHERE\n",
    "    video.url IN (SELECT DISTINCT(url) FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions`)\n",
    "    AND\n",
    "    video.url IN (SELECT DISTINCT(video_url) AS url FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings`)\n",
    "    AND\n",
    "    video.id NOT IN (SELECT id FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_video_metadata`)\n",
    ") video\n",
    "ON\n",
    "  video.url = transcription.url\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "transcriptions_df = read_gbq(transcriptions_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`embeddings`**\n",
    "Next up, the `embeddings` table! This one will require a *little* more setup, since we'll need to separately download the embeddings themselves from GCS. (My GBQ dataset only has pointers to each of the GCS URLs.)\n",
    "\n",
    "I'll start by downloading all of the GBQ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:09:37,656 - postgres_notebook - INFO - DETERMINING DELTA FOR embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DETERMINING DELTA FOR embeddings\n",
    "# ================================\n",
    "# Below, I'm going to determine the delta between the current state of the Postgres database and the state of the GBQ database.\n",
    "\n",
    "# Log that we're determining the delta for the embeddings table\n",
    "logger.info(\"DETERMINING DELTA FOR embeddings...\")\n",
    "\n",
    "# Declare the query that will download the `embeddings` table\n",
    "embeddings_query = f\"\"\"\n",
    "SELECT \n",
    "  embedding.id,\n",
    "  embedding.video_url AS url,\n",
    "  embedding.embedding_type,\n",
    "  embedding.start_segment,\n",
    "  embedding.end_segment,\n",
    "  embedding.segment_length,\n",
    "  embedding.gcs_uri\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings` embedding\n",
    "WHERE\n",
    "  embedding.id NOT IN (SELECT id FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_embeddings`)\n",
    "  AND embedding.video_url IN (\n",
    "    SELECT DISTINCT(video_url) \n",
    "    FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings` \n",
    "    LIMIT {max_n_videos_to_update_embeddings}\n",
    "  )\n",
    "GROUP BY\n",
    "  embedding.id,\n",
    "  embedding.video_url,\n",
    "  embedding.embedding_type,\n",
    "  embedding.start_segment,\n",
    "  embedding.end_segment,\n",
    "  embedding.segment_length,\n",
    "  embedding.gcs_uri\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "# TODO: UNCOMMENT THIS .head(1000) TO GET ALL OF THE EMBEDDINGS\n",
    "embeddings_df = read_gbq(embeddings_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can delete the temporary tables now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:09:47,090 - postgres_notebook - INFO - DELETING TEMPORARY GBQ TABLES...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: |\u001b[32m          \u001b[0m|\n",
      "Downloading: |\u001b[32m          \u001b[0m|\n",
      "Downloading: |\u001b[32m          \u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "# Indicate that we're deleting the temporary GBQ tables\n",
    "logger.info(\"DELETING TEMPORARY GBQ TABLES...\")\n",
    "\n",
    "# Delete each of the cur_pg_db tables\n",
    "try:\n",
    "    read_gbq(f\"DROP TABLE IF EXISTS `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_video_metadata`\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    read_gbq(f\"DROP TABLE IF EXISTS `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_transcriptions`\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    read_gbq(f\"DROP TABLE IF EXISTS `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.cur_pg_db_embeddings`\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got all of the embeddings metadata, I can download the embeddings themselves. I'll start by creating a temporary directory to store embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:10:12,269 - postgres_notebook - INFO - DOWNLOADING EMBEDDINGS FROM GCS...\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# DOWNLOADING EMBEDDINGS FROM GCS\n",
    "# ===============================\n",
    "# Below, I'm going to download the embeddings from GCS to the local machine\n",
    "\n",
    "# Log that we're downloading the embeddings from GCS\n",
    "logger.info(\"DOWNLOADING EMBEDDINGS FROM GCS...\")\n",
    "\n",
    "# Create a temporary directory to store the embeddings\n",
    "temp_emb_directory_path = Path(\"temp_embeddings\")\n",
    "temp_emb_directory_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Remove any files that're already in the directory if it exists\n",
    "for file in temp_emb_directory_path.glob(\"*\"):\n",
    "    file.unlink()\n",
    "\n",
    "# Create a GCS client\n",
    "gcs_client = storage.Client(\n",
    "    project=GBQ_PROJECT_ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll iterate through each of the GCS URIs and download them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3177/3177 [00:45<00:00, 70.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the list of GCS URIs\n",
    "gcs_uris = embeddings_df[\"gcs_uri\"].unique()\n",
    "\n",
    "def download_embedding(idx_and_uri):\n",
    "    idx, gcs_uri = idx_and_uri\n",
    "    try:\n",
    "        # Parse the GCS URI\n",
    "        split_gcs_uri = gcs_uri.split(\"gs://\")[-1]\n",
    "        bucket_name, file_name = split_gcs_uri.split(\"/\")[0], \"/\".join(\n",
    "            split_gcs_uri.split(\"/\")[1:]\n",
    "        )\n",
    "        \n",
    "        # Download the embedding corresponding with this GCS URI\n",
    "        download_file_from_bucket(\n",
    "            bucket_name=bucket_name,\n",
    "            file_name=file_name,\n",
    "            destination_folder=str(temp_emb_directory_path) + \"/\",\n",
    "            project_id=GBQ_PROJECT_ID,\n",
    "            gcs_client=gcs_client,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing GCS URI: {e}\")\n",
    "        pass\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the download process\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(download_embedding, idx_uri): idx_uri for idx_uri in enumerate(gcs_uris)}\n",
    "    for future in tqdm(as_completed(futures), total=len(gcs_uris)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll load all of the embeddings into RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:11:16,136 - postgres_notebook - INFO - LOADING EMBEDDINGS INTO POSTGRES...\n",
      "100%|██████████| 3177/3177 [00:26<00:00, 119.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOADING EMBEDDINGS INTO RAM\n",
    "# ===========================\n",
    "# Below, I'm going to load the embeddings into RAM, so that I can then upload them to the Postgres database\n",
    "\n",
    "# Log that we're loading the embeddings into the Postgres database\n",
    "logger.info(\"LOADING EMBEDDINGS INTO RAM...\")\n",
    "\n",
    "# We're going to store the embeddings in a dictionary, where the key is the\n",
    "# embedding ID and the value is the ndarray of the embedding\n",
    "embeddings = {}\n",
    "for idx, emb_file in tqdm(list(enumerate(list(temp_emb_directory_path.iterdir())))):\n",
    "    try:\n",
    "        # Load in the .npy file as a numpy array\n",
    "        embedding = np.load(emb_file)\n",
    "        \n",
    "        # If the embedding is empty, skip it\n",
    "        # TODO: This is because it seems like a ton of embeddings are \n",
    "        # empty. We should figure out why that is.\n",
    "        if embedding.shape == ():\n",
    "            continue\n",
    "        \n",
    "        # Get the embedding ID\n",
    "        embedding_id = emb_file.stem\n",
    "        \n",
    "        # Add the embedding to the dictionary, storing the list representation\n",
    "        embeddings[embedding_id] = embedding.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've loaded all of the embeddings, I'll delete all of the files in the temporary folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the files in the temp directory\n",
    "for file in temp_emb_directory_path.glob(\"*\"):\n",
    "    file.unlink()\n",
    "    \n",
    "# Delete the temp directory\n",
    "temp_emb_directory_path.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll add the embeddings I've loaded to the embedding DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"loaded_embeddings_df\" that has the embeddings loaded in\n",
    "loaded_embeddings_df = embeddings_df.copy()\n",
    "loaded_embeddings_df[\"embedding\"] = loaded_embeddings_df[\"id\"].apply(\n",
    "    lambda x: embeddings.get(x, None)\n",
    ")\n",
    "\n",
    "# Drop any rows where the embedding is None\n",
    "loaded_embeddings_df = loaded_embeddings_df.dropna(subset=[\"embedding\"]).drop_duplicates(\n",
    "    subset=[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to the Postgres Database\n",
    "Now that I've downloaded all of the data, I'm going to determine which data I need to upload, and then upload it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the data currently in the database, we'll figure out which files we need to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:12:48,909 - postgres_notebook - INFO - ADDING ROWS TO POSTGRES TABLES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:12:48,956 - postgres_notebook - INFO - Adding 56 rows to the video_metadata table.\n",
      "2024-02-18 23:12:48,958 - postgres_notebook - INFO - Adding 9476 rows to the transcriptions table.\n",
      "2024-02-18 23:12:48,958 - postgres_notebook - INFO - Adding 3177 rows to the embeddings table.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# ADDING ROWS TO POSTGRES TABLES\n",
    "# ==============================\n",
    "# Below, I'm going to add the rows to the Postgres tables\n",
    "\n",
    "# Log that we're adding rows to the Postgres tables\n",
    "logger.info(\"ADDING ROWS TO POSTGRES TABLES...\")\n",
    "\n",
    "# Determine which rows of the video_metadata_df we need to add to the database\n",
    "video_metadata_df_to_add = video_metadata_df[\n",
    "    ~video_metadata_df[\"id\"].isin(cur_database_video_metadata_df[\"id\"])\n",
    "].copy()\n",
    "\n",
    "# Determine which rows of the transcriptions_df we need to add to the database\n",
    "transcriptions_df_to_add = transcriptions_df[\n",
    "    ~transcriptions_df[\"url\"].isin(cur_database_transcriptions_df[\"url\"])\n",
    "].copy()\n",
    "\n",
    "# Determine which rows of the embeddings_df we need to add to the database\n",
    "embeddings_df_to_add = loaded_embeddings_df[\n",
    "    ~loaded_embeddings_df[\"id\"].isin(cur_database_embeddings_df[\"id\"])\n",
    "].copy()\n",
    "\n",
    "# Log some information about the number of rows we're adding\n",
    "logger.info(\n",
    "    f\"Adding {len(video_metadata_df_to_add)} rows to the video_metadata table.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Adding {len(transcriptions_df_to_add)} rows to the transcriptions table.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Adding {len(embeddings_df_to_add)} rows to the embeddings table.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to Postgres Table\n",
    "Now, we'll upload each of the DataFrames to the Postgres server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the video metadata to the database\n",
    "upload_to_table(\n",
    "    video_metadata_df_to_add,\n",
    "    \"video_metadata\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Upload the transcriptions to the database\n",
    "upload_to_table(\n",
    "    transcriptions_df_to_add,\n",
    "    \"transcriptions\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Upload the embeddings to the database\n",
    "upload_to_table(\n",
    "    embeddings_df_to_add.drop(columns=[\"gcs_uri\"]),\n",
    "    \"embeddings\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    "    chunksize=postgres_upload_chunksize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Indices\n",
    "Below, I'm going to recreate the table indices. This will ensure that I'm able to create fast queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:25:39,360 - postgres_notebook - INFO - RE-CREATING THE EMBEDDINGS INDEX...\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# EMBEDDINGS INDEX CREATION\n",
    "# =========================\n",
    "# Below, I'm going to create the index for the embeddings table\n",
    "\n",
    "query_postgres(\n",
    "    f\"SET max_parallel_maintenance_workers = {postgres_max_parallel_maintenance_workers}; -- plus leader\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "query_postgres(\n",
    "    f\"SET maintenance_work_mem = '{postgres_maintenance_work_mem}';\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# If the user wants to re-create the embeddings index, then do so\n",
    "if recreate_embeddings_index_if_exists:\n",
    "    # Log that we're re-creating the embeddings index\n",
    "    logger.info(\"RE-CREATING THE EMBEDDINGS INDEX...\")\n",
    "\n",
    "    # Drop the index if it already exists\n",
    "    query_postgres(\n",
    "        \"DROP INDEX IF EXISTS embeddings_embedding_idx;\",\n",
    "        engine=engine,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Run the query that will create the IVFFlat index\n",
    "    query_postgres(\n",
    "        f\"\"\"CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {embeddings_index_ivfflat_nlist});\"\"\",\n",
    "        engine=engine,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Try to create the index if it doesn't exist\n",
    "    try:\n",
    "        # Log that we're creating the embeddings index\n",
    "        logger.info(\"CREATING THE EMBEDDINGS INDEX...\")\n",
    "\n",
    "        # Run the query that will create the IVFFlat index\n",
    "        query_postgres(\n",
    "            f\"\"\"CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {embeddings_index_ivfflat_nlist});\"\"\",\n",
    "            engine=engine,\n",
    "            logger=logger,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Log the error if the index already exists\n",
    "        logger.error(f\"An error occurred while creating the embeddings index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the `embeddings_to_text` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CREATING EMBEDDINGS TO TEXT\n",
    "# ===========================\n",
    "# Below, I'm going to create a table that maps embeddings to text\n",
    "\n",
    "table_creation_query = f\"\"\"\n",
    "CREATE TABLE embeddings_to_text AS (\n",
    "    WITH \n",
    "    embedding_to_text_flattened AS (\n",
    "        SELECT\n",
    "        embeddings.id,\n",
    "        embeddings.url,\n",
    "        embeddings.start_segment,\n",
    "        embeddings.end_segment,\n",
    "        transcriptions.text,\n",
    "        transcriptions.segment_id,\n",
    "        transcriptions.segment_start,\n",
    "        transcriptions.segment_end\n",
    "        FROM\n",
    "        embeddings\n",
    "        LEFT JOIN\n",
    "        transcriptions\n",
    "        ON\n",
    "        transcriptions.segment_id >= embeddings.start_segment\n",
    "        AND\n",
    "        transcriptions.segment_id < embeddings.end_segment\n",
    "        AND\n",
    "        transcriptions.url = embeddings.url\n",
    "        ORDER BY\n",
    "        url DESC,\n",
    "        segment_id ASC\n",
    "    ),\n",
    "    \n",
    "    embedding_to_text AS (\n",
    "        SELECT\n",
    "            id,\n",
    "            url,\n",
    "            start_segment,\n",
    "            end_segment,\n",
    "            ARRAY_TO_STRING(ARRAY_AGG(emb.text ORDER BY emb.segment_id), '') AS text,\n",
    "            MIN(emb.segment_start) AS segment_start,\n",
    "            MAX(emb.segment_end) AS segment_end\n",
    "        FROM\n",
    "            embedding_to_text_flattened emb\n",
    "        GROUP BY\n",
    "            id,\n",
    "            url,\n",
    "            start_segment,\n",
    "            end_segment\n",
    "    )\n",
    "\n",
    "    SELECT * FROM embedding_to_text\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "query_postgres(\n",
    "    table_creation_query,\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
