{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uploading Data to Postgres Tables**\n",
    "Now that I've spent some time initializing the tables, I can upload additional data to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up some envvars\n",
    "%env LOG_TO_CONSOLE=True\n",
    "%env LOG_LEVEL=INFO\n",
    "%env TQDM_ENABLED=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import pandas as pd\n",
    "from pandas_gbq import read_gbq\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "# Importing modules custom-built for this project\n",
    "from utils.settings import (\n",
    "    GBQ_PROJECT_ID,\n",
    "    GBQ_DATASET_ID,\n",
    "    POSTGRES_USER,\n",
    "    POSTGRES_PASSWORD,\n",
    "    POSTGRES_HOST,\n",
    "    POSTGRES_PORT,\n",
    "    POSTGRES_DB,\n",
    "    LOG_TO_CONSOLE,\n",
    ")\n",
    "from utils.logging import get_logger\n",
    "from utils.gcs import download_file_from_bucket\n",
    "from utils.postgres import query_postgres, upload_to_table\n",
    "\n",
    "# Set up a logger for this notebook\n",
    "logger = get_logger(\"postgres_notebook\", log_to_console=LOG_TO_CONSOLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to set up the Postgres engine via SQLAlchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string to the database\n",
    "postgres_connection_string = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(postgres_connection_string)\n",
    "metadata = MetaData()\n",
    "session = sessionmaker(bind=engine)()\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to set up the Postgres engine via SQLAlchemy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Downloading Data from GBQ**\n",
    "\n",
    "Before I do anything with `postgres`, I'm just going to download all of the data from GBQ. This will save me some time now (since I can more easily check the Postgres table to understand which data to upload), but I should probably change this in the future to optimize performance & speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`video_metadata`**\n",
    "The first table is the `video_metadata` table. I'll download that (and some columns from the `enriched_video_metadata` table) for all of the videos that I've transcribed and embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query that'll grab all of the video metadata from the GBQ database\n",
    "video_metadata_query = f\"\"\"\n",
    "-- This query will select metadata for all of the videos that have transcriptions & embeddings\n",
    "SELECT\n",
    "  video.*,\n",
    "  enriched_video.video_type,\n",
    "  enriched_video.review_score\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.video_metadata` video\n",
    "JOIN\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.enriched_video_metadata` enriched_video\n",
    "ON\n",
    "  video.id = enriched_video.id\n",
    "WHERE\n",
    "  video.url IN (SELECT DISTINCT(url) FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions`)\n",
    "  AND\n",
    "  video.url IN (SELECT DISTINCT(video_url) AS url FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings`)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "video_metadata_df = read_gbq(video_metadata_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`transcriptions`**\n",
    "Next, I'm going to download all of the transcriptions for the videos I'd identified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the query that will download all of the relevant rows from the \n",
    "# transcription table\n",
    "transcriptions_query = f\"\"\"\n",
    "SELECT \n",
    "  transcription.url,\n",
    "  transcription.text,\n",
    "  transcription.segment_id,\n",
    "  transcription.segment_seek,\n",
    "  transcription.segment_start,\n",
    "  transcription.segment_end\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions` transcription\n",
    "JOIN (\n",
    "  SELECT\n",
    "    video.url\n",
    "  FROM\n",
    "    `neural-needledrop.backend_data.video_metadata` video\n",
    "  WHERE\n",
    "    video.url IN (SELECT DISTINCT(url) FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions`)\n",
    "    AND\n",
    "    video.url IN (SELECT DISTINCT(video_url) AS url FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings`)\n",
    ") video\n",
    "ON\n",
    "  video.url = transcription.url\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "transcriptions_df = read_gbq(transcriptions_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`embeddings`**\n",
    "Next up, the `embeddings` table! This one will require a *little* more setup, since we'll need to separately download the embeddings themselves from GCS. (My GBQ dataset only has pointers to each of the GCS URLs.)\n",
    "\n",
    "I'll start by downloading all of the GBQ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the query that will download the `embeddings` table\n",
    "embeddings_query = f\"\"\"\n",
    "SELECT \n",
    "  embedding.id,\n",
    "  embedding.video_url AS url,\n",
    "  embedding.embedding_type,\n",
    "  embedding.start_segment,\n",
    "  embedding.end_segment,\n",
    "  embedding.segment_length,\n",
    "  embedding.gcs_uri\n",
    "FROM\n",
    "  `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings` embedding\n",
    "JOIN (\n",
    "  SELECT\n",
    "    video.url\n",
    "  FROM\n",
    "    `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.video_metadata` video\n",
    "  WHERE\n",
    "    video.url IN (SELECT DISTINCT(url) FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.transcriptions`)\n",
    "    AND\n",
    "    video.url IN (SELECT DISTINCT(video_url) AS url FROM `{GBQ_PROJECT_ID}.{GBQ_DATASET_ID}.embeddings`)\n",
    ") video\n",
    "ON\n",
    "  video.url = embedding.video_url\n",
    "GROUP BY\n",
    "  embedding.id,\n",
    "  embedding.video_url,\n",
    "  embedding.embedding_type,\n",
    "  embedding.start_segment,\n",
    "  embedding.end_segment,\n",
    "  embedding.segment_length,\n",
    "  embedding.gcs_uri\n",
    "\"\"\"\n",
    "\n",
    "# Execute the above query\n",
    "# TODO: UNCOMMENT THIS .head(1000) TO GET ALL OF THE EMBEDDINGS\n",
    "embeddings_df = read_gbq(embeddings_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got all of the embeddings metadata, I can download the embeddings themselves. I'll start by creating a temporary directory to store embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to store the embeddings\n",
    "temp_emb_directory_path = Path(\"temp_embeddings\")\n",
    "temp_emb_directory_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Remove any files that're already in the directory if it exists\n",
    "for file in temp_emb_directory_path.glob(\"*\"):\n",
    "    file.unlink()\n",
    "\n",
    "# Create a GCS client\n",
    "gcs_client = storage.Client(\n",
    "    project=GBQ_PROJECT_ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll iterate through each of the GCS URIs and download them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the list of GCS URIs\n",
    "gcs_uris = embeddings_df[\"gcs_uri\"].unique()\n",
    "\n",
    "def download_embedding(idx_and_uri):\n",
    "    idx, gcs_uri = idx_and_uri\n",
    "    try:\n",
    "        # Parse the GCS URI\n",
    "        split_gcs_uri = gcs_uri.split(\"gs://\")[-1]\n",
    "        bucket_name, file_name = split_gcs_uri.split(\"/\")[0], \"/\".join(\n",
    "            split_gcs_uri.split(\"/\")[1:]\n",
    "        )\n",
    "        \n",
    "        # Download the embedding corresponding with this GCS URI\n",
    "        download_file_from_bucket(\n",
    "            bucket_name=bucket_name,\n",
    "            file_name=file_name,\n",
    "            destination_folder=str(temp_emb_directory_path) + \"/\",\n",
    "            project_id=GBQ_PROJECT_ID,\n",
    "            gcs_client=gcs_client,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing GCS URI: {e}\")\n",
    "        pass\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the download process\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(download_embedding, idx_uri): idx_uri for idx_uri in enumerate(gcs_uris)}\n",
    "    for future in tqdm(as_completed(futures), total=len(gcs_uris)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll load all of the embeddings into RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to store the embeddings in a dictionary, where the key is the\n",
    "# embedding ID and the value is the ndarray of the embedding\n",
    "embeddings = {}\n",
    "for idx, emb_file in tqdm(list(enumerate(list(temp_emb_directory_path.iterdir())))):\n",
    "    try:\n",
    "        # Load in the .npy file as a numpy array\n",
    "        embedding = np.load(emb_file)\n",
    "        \n",
    "        # If the embedding is empty, skip it\n",
    "        # TODO: This is because it seems like a ton of embeddings are \n",
    "        # empty. We should figure out why that is.\n",
    "        if embedding.shape == ():\n",
    "            continue\n",
    "        \n",
    "        # Get the embedding ID\n",
    "        embedding_id = emb_file.stem\n",
    "        \n",
    "        # Add the embedding to the dictionary, storing the list representation\n",
    "        embeddings[embedding_id] = embedding.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've loaded all of the embeddings, I'll delete all of the files in the temporary folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the files in the temp directory\n",
    "for file in temp_emb_directory_path.glob(\"*\"):\n",
    "    file.unlink()\n",
    "    \n",
    "# Delete the temp directory\n",
    "temp_emb_directory_path.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll add the embeddings I've loaded to the embedding DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"loaded_embeddings_df\" that has the embeddings loaded in\n",
    "loaded_embeddings_df = embeddings_df.copy()\n",
    "loaded_embeddings_df[\"embedding\"] = loaded_embeddings_df[\"id\"].apply(\n",
    "    lambda x: embeddings.get(x, None)\n",
    ")\n",
    "\n",
    "# Drop any rows where the embedding is None\n",
    "loaded_embeddings_df = loaded_embeddings_df.dropna(subset=[\"embedding\"]).drop_duplicates(\n",
    "    subset=[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data to the Postgres Database\n",
    "Now that I've downloaded all of the data, I'm going to determine which data I need to upload, and then upload it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Data to Upload\n",
    "I need to identify which data is already in each of the `postgres` tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the videos currently in the `video_metadata` table\n",
    "cur_database_video_metadata_df = query_postgres(\n",
    "    \"SELECT id FROM video_metadata\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Determine the transcriptions currently in the `transcriptions` table\n",
    "cur_database_transcriptions_df = query_postgres(\n",
    "    \"SELECT DISTINCT(url) FROM transcriptions\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Determine the embeddings currently in the `embeddings` table\n",
    "cur_database_embeddings_df = query_postgres(\n",
    "    \"SELECT DISTINCT(id) FROM embeddings\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the data currently in the database, we'll figure out which files we need to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which rows of the video_metadata_df we need to add to the database\n",
    "video_metadata_df_to_add = video_metadata_df[\n",
    "    ~video_metadata_df[\"id\"].isin(cur_database_video_metadata_df[\"id\"])\n",
    "].copy()\n",
    "\n",
    "# Determine which rows of the transcriptions_df we need to add to the database\n",
    "transcriptions_df_to_add = transcriptions_df[\n",
    "    ~transcriptions_df[\"url\"].isin(cur_database_transcriptions_df[\"url\"])\n",
    "].copy()\n",
    "\n",
    "# Determine which rows of the embeddings_df we need to add to the database\n",
    "embeddings_df_to_add = loaded_embeddings_df[\n",
    "    ~loaded_embeddings_df[\"id\"].isin(cur_database_embeddings_df[\"id\"])\n",
    "].copy()\n",
    "\n",
    "# Log some information about the number of rows we're adding\n",
    "logger.info(\n",
    "    f\"Adding {len(video_metadata_df_to_add)} rows to the video_metadata table.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Adding {len(transcriptions_df_to_add)} rows to the transcriptions table.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Adding {len(embeddings_df_to_add)} rows to the embeddings table.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to Postgres Table\n",
    "Now, we'll upload each of the DataFrames to the Postgres server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the video metadata to the database\n",
    "upload_to_table(\n",
    "    video_metadata_df_to_add,\n",
    "    \"video_metadata\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Upload the transcriptions to the database\n",
    "upload_to_table(\n",
    "    transcriptions_df_to_add,\n",
    "    \"transcriptions\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Upload the embeddings to the database\n",
    "upload_to_table(\n",
    "    embeddings_df_to_add.drop(columns=[\"gcs_uri\"]),\n",
    "    \"embeddings\",\n",
    "    engine=engine,\n",
    "    logger=logger,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
